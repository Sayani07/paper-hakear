---
title: A new metric for automatic discovery of periodic patterns in time series

# to produce blinded version set to 1
blinded: 0

authors:
- name: Sayani Gupta
  thanks: "Email: Sayani.Gupta@monash.edu"
  affiliation: Department of Econometrics and Business Statistics, Monash University, Australia

- name: Rob J Hyndman
  affiliation: Department of Econometrics and Business Statistics, Monash University, Australia

- name: Dianne Cook
  affiliation: Department of Econometrics and Business Statistics, Monash University, Australia
keywords:
- data visualization
- periodicities
- time granularities
- cyclic granularities
- permutation tests
- Jensen-Shannon distances
- smart meter data
- R

abstract: |
 Periodic patterns or associations in large univariate time series data could be discerned by analysing the behaviour across cyclic temporal granularities, which are temporal deconstructions accounting for repetitive behaviour. The temporal granularities form ordered categorical variables and display of distributions of the univariate response variable across two or more combinations of these categorical variable can help explore periodicities, patterns and anomalies. A  pair of granularities that can be meaningfully examined together are called “harmonies” and the ones which cannot be are called “clashes”. Even after excluding clashes, the list of harmonies that could potentially be displayed is huge and hence overwhelming for human consumption. This work provides a methodology to screen the most informative graphics from the plethora of choices by introducing a distance measure that could be compared across harmonies with varied levels and data sets. Moreover, this distance measure could also be used to rank the selected harmonies basis how well they capture the variation in the measured variable. All the methods are implemented in the open source R package `hakear`. 

bibliography: [bibliography.bib]
preamble: >
  \usepackage{mathtools,amssymb,booktabs,amsthm,todonotes,colortbl}
  \def\mod{~\text{mod}~}
  \newtheorem{definition}{Definition}
  \usepackage{mathptmx}
  \usepackage{caption}
  \DeclareCaptionStyle{italic}{labelfont={bf},textfont={it},labelsep=colon}
  \captionsetup[figure]{style=italic,format=hang,singlelinecheck=true}
  \captionsetup[table]{style=italic,format=hang,singlelinecheck=true}
  \def\novspacing{\setlength{\aboverulesep}{0pt}\setlength{\belowrulesep}{0pt}}
  \def\vspacing{\setlength{\aboverulesep}{0.4ex}\setlength{\belowrulesep}{0.65ex}}
output:
  bookdown::pdf_book:
    base_format: rticles::asa_article
    fig_height: 5
    fig_width: 8
    fig_caption: yes
    dev: "pdf"
    keep_tex: yes
---


```{r initial, echo = FALSE, cache = FALSE, include = FALSE}
options("knitr.graphics.auto_pdf" = TRUE,
        tinytex.verbose = FALSE)
library(knitr)
library(tidyverse)
library(lubridate)
library(lvplot)
library(ggridges)
library(viridis)
library(tsibble)
library(gravitas)
library(ggpubr)
library(readr)
library(kableExtra)
library(distributional)
library(ggplot2)
library(sugrrants)
library(here)
library(ggplot2)
library(patchwork)

opts_chunk$set(
  echo = FALSE, warning = FALSE, message = FALSE, comment = "#>",
  fig.path = "figure/", fig.align = "center", fig.show = "hold",
  cache = TRUE, cache.path = "cache/",
  out.width = ifelse(is_html_output(), "100%", "\\textwidth")
)
knitr::opts_knit$set(root.dir = here::here())
#                                                                read_chunk('main.R')
here::here()
#knitr::read_chunk(here::here("paper/R/", "null_distribution.R"))
#knitr::read_chunk(here::here("paper/R/", "mean_null.R"))
#source(here::here("paper/R/sim_panel.R"))
# Set up plan
#source("_drake.R")
# Run all code required
#drake::r_make()
# load each object as required
# loadd()
```



```{r}
set.seed(321)
```


# Introduction

<!-- <introducing the problem> -->
<!-- background of the problem -->

Exploratory data analysis, as coined by John W. Tukey (Tukey 1965) involves many iterations of finding structures and patterns that allow the data to be informative. With temporal data available at finer scales, exploring periodicity and their relationships can become overwhelming with so many possible cyclic temporal granularities [@Gupta2020-vo] to explore. 
\noindent Take the example of the calendar display of electricity smart meter data in Figure (\ref{fig:calendar-elec}) used in @wang2020calendar for four households in Melbourne, Australia. The authors show how hour-of-the-day interact with weekdays and weekends and then move on to use calendar display to show daily schedules. The calendar display has several components in it, which helps us to look at energy consumption across hour-of-the-day, day-of-the-week, week-of-the-month, and month-of-the-year at once. Some interaction of these cyclic granularities, for example, how  day-of-week relates to month-of-year, could also be interpreted from this display. This is a great way for having an overview of energy consumption. However, if one wants to understand the periodicities in energy behavior and how the periodicities interact in greater detail, it is not easy to comprehend the interactions of some periodicities' from this display, due to the combination of linear and cyclic representation of time. For example, this display might not be the best to understand how hour-of-the-day or month-of-year varies across week-of-the-month as well as with each other. Furthermore, it is not clear what all interactions of cyclic granularities should be read from this display as there could be many combinations that one can look at. Moreover, _"calendar effects are not restricted to conventional day-of-week or month-of-year deconstructions"_ (@Gupta2020-vo) and could include other cyclic granularities like hour-of-week or day-of-fortnight, which could potentially become useful depending on the context. Possible areas where it is useful are monitoring heart rates which could record number of heartbeats every minute or analyze web search data for which data is available for a temporal scale as fine as second.


```{r calendar-elec, fig.height = 8, fig.cap = "Calendar displays faceted by two households are shown. The calendar displays are rich in information and has several components. For example, it can be observed easily that the level of energy consumption by household 2 is higher than household 4. Also, it gives an overview on when these households are away for holidays and which months consumptions are highest (Jan, Feb). However,when it comes to discerning periodic patterns, analyzing all possible periodic patterns through this display is overwhelming due to combination of linear and cyclic representation of time. For example, there is more going on in terms of peaks and troughs in id 2 compared to id 4, but it is not clear which all periodic patterns are dominant and if it differs between households."}
elec <- read_rds(here("paper/data/elec.rds")) %>% 
  filter(date >= ymd("20180101"), date < ymd("20180701"))
rdbl <- c("Weekday" = "#d7191c", "Weekend" = "#2c7bb6")

elec <- elec %>% 
  mutate(
    wday = wday(date, label = TRUE, week_start = 1),
    weekday = if_else(wday %in% c("Sat", "Sun"), "Weekend", "Weekday")
  )
p_cal_elec <- elec %>% 
  filter(id %in% c(2, 4)) %>% 
  frame_calendar(x = time, y = kwh, date = date, nrow = 1) %>% 
    ggplot(aes(x = .time, y = .kwh, group = date)) +
    geom_line(aes(colour = as.factor(id)), size = 0.5) +
    scale_colour_brewer(name = "", palette = "Dark2", direction = 1) +
    facet_grid(id ~ ., labeller = label_both) +
    theme(legend.position = "bottom")
prettify(p_cal_elec, size = 2.5, label.padding = unit(0.1, "lines"))
```


<!-- context when it could be useful is monitoring heart rates. There are devices that can detect each heartbeat and transfer the data to a receiver such as a watch or phone. These data could be available for a temporal scale as fine as a minute and it could be of interest to see regular patterns across any deconstruction of time coarser than a minute. -->

```{r intro_all}

id2_tsibble <- elec %>% 
   filter(id == 2) %>% 
   as_tsibble(index = date_time)
  
id4_tsibble <- elec %>% 
   filter(id == 4) %>% 
   as_tsibble(index = date_time)

# hour-of-day and month-of-year (important pair) id2's behavior across different hours of the day very different across months, but for id4 behavior across different hours of the day is not likely a function of month.
p1 <- id2_tsibble %>%
   prob_plot("month_year",
             "hour_day",
             response = "kwh",
             plot_type = "quantile",
             symmetric = TRUE,
             quantile_prob = c(0.1, 0.25,0.5,0.75, 0.9)) +
  ggtitle("") + theme(
        strip.text = element_text(size = 10, margin = margin(b = 0, t = 0))) + 
  scale_colour_brewer(name = "", palette = "PiYG")

p2 <- id4_tsibble %>%
   prob_plot("month_year",
             "hour_day",
             response = "kwh",
             plot_type = "quantile",
             symmetric = TRUE,
             quantile_prob = c(0.1, 0.25,0.5,0.75, 0.9)) + 
  ggtitle("a") + theme(
        strip.text = element_text(size = 10, margin = margin(b = 0, t = 0)))


p3 <- id2_tsibble %>%
  create_gran("week_month") %>% 
  filter(week_month != 5) %>% 
   prob_plot("wknd_wday",
             "week_month",
             response = "kwh",
             plot_type = "quantile",
             symmetric = FALSE,
             quantile_prob = c(0.25,0.5,0.75)) +
   ggtitle("") +

        theme(strip.text = element_text(size = 10, margin = margin(b = 0, t = 0)))

p4 <- id4_tsibble %>%
    create_gran("week_month") %>% 
  filter(week_month != 5) %>% 
   prob_plot("wknd_wday",
             "week_month",
             response = "kwh",
             plot_type = "quantile",
             symmetric = FALSE,
             quantile_prob = c(0.25,0.5,0.75)) +
  ggtitle("b") +  #+ scale_x_discrete(breaks = seq(0, 23, 4)) + 
theme(
        strip.text = element_text(size = 10, margin = margin(b = 0, t = 0)))

```

```{r id2, fig.cap = "Distribution of energy consumption displayed through area quantile plots across two cyclic granularities month-of-year and hour-of-day for id2 (a) and id4 (b). The black line is the median, whereas the purple band covers the 25th to 75th percentile, the orange band covers the 10th to 90th percentile, and the green band covers the 1st to 99th percentile. Difference between 90th and 75th quantiles less for (Jan, Feb) in a) suggesting that it is a more frequent user of air conditioner than b). Energy consumption in a) changes across both granularities, whereas for b) daily pattern stays same irrespective of the months."}
ggpubr::ggarrange(p1, p2, ncol = 2)
```

```{r id4, fig.cap = "something2", eval = FALSE}
ggpubr::ggarrange(p3, p4, ncol = 2)
```

Even when it is known which all interactions to look at, all of them would not be interesting and that too will vary across different households. For example, area distribution quantiles are plotted for household 2 and 4 in Figure \ref{fig:id2}a and \ref{fig:id2}b respectively across month-of-year on facets and hour-of-day on x-axis. For the first household, the 75th and 90th percentile for January and February are very close, implying that energy usage for these months are generally on a much higher side, possibly due to the usage of air conditioners (January and February are peak summer in Australia), however for other months (Autumn and winter), the difference between the percentiles are not high, implying this household do not use so much heater as compared to air conditioner. Also, a lot of households in Victoria use gas heating and hence the usage of heaters might not be reflected here. The energy consumption for household 2 is also higher in summer months relative to autumn or winter, but the 75th and 90th percentile are far apart in all months, implying that the second household resorts to air conditioners much less regularly than the first one. Moreover, the 75th percentile distribution does not follow the same pattern as 90th percentile for all months for the first household, whereas, the pattern looks pretty similar for all months for the second household. Difference in the energy consumption could vary both across month-of-year (facets) and hour-of-day (x-axis). For the first household, both these cyclic granularities would deem important. Although, it seems like energy consumption across hours of the day are not that different across different months for the second household or atleast differences seem to be more prominent across month-of-year (facets) than hour-of-day (x-axis). It could be immensely useful to make the transition from all possible ways to only ways that could potentially be informative.


<!-- Again, look at \ref{fig:} c and d, where energy consumption for these two households are plotted against (weekend/weekday, week-of-month). Here, for both households, the pattern of energy consumption vary  across different weeks of the month irrespective of the fact it is a weekday or weekend. In that respect, the harmony pair (month-of-year, hour-of-day) seems to be more informative than (weekend/weekday, week-of-month) for the first household.  -->


<!-- Take an example of a data set which are observed at fine temporal scales, like that of NYC bike usage available at https://www.citibikenyc.com/system-data. We use the `nyc_bikes` data set from the R package `tsibbledata` which takes a sample of 10 bikes for the year 2018. The `start_time` and the `stop_time` are recorded to a fineness of seconds. We can look at pair of cyclic granularities (hour_day, wknd_wday) or (week_month, day_week) to see how these periodicities interact. But there could be other pairs that are important too. How to understand which pairs are sufficient to explore given the data set without losing much information about the data. -->

<!-- When we need to understand the interplay of different periodicities in a high frequency temporal datasets, we have many choices to consider. In [@wang2019tsibble] and [@wang2020calendar], periodicities are explored across hour of the day and day of the week or months. But calendar effects are not restricted to conventional day-of-week or month-of-year deconstructions. -->


<!-- If we have $n$ periodic linear granularities in the hierarchy table, then $n(n-1)/2$ circular or quasi-circular cyclic granularities could be constructed.  -->

<!-- what is the dimension of the problem -->
The paper @Gupta2020-vo describes how we can compute all possible combinations of cyclic time granularities. Let $N_C$ be the total number of contextual circular, quasi-circular and aperiodic cyclic granularities that can originate from the underlying linear granularities. The graphical mapping is such that distributions of a numeric response variable is displayed across combinations of cyclic granularities, one placed at x-axis and the other on the facet. That essentially implies there are $^{N_C}P_2$ possible pairwise plots exhaustively, where each plot would display a pair of cyclic granularities. This is large and overwhelming for human consumption.<!-- Scagnostics literature --> This problem is similar to Scagnostics (Scatterplot Diagnostics) by @tukey1988computer, which is used to discern meaningful patterns in large collections of scatterplots. Given a set of $v$ variables, there are $v(v-1)/2$ pairs of variables, and thus the same number of possible pairwise scatterplots. Therefore, even for small $v$, the number of scatterplots can be large, and scatterplot matrices (SPLOMs) could easily run out of pixels when presenting high-dimensional data. @Dang2014-tw and @wilkinson2005graph provides potential solutions to this, where few characterizations help us to locate anomalies in density, shape, trend, and other features in the 2D point scatters. 




<!-- harmonies and why it is not enough -->
This work is a natural extension of our work (@Gupta2020-vo), which narrows down the search from $^{N_C}P_2$ plots by identifying pairs of granularities that can be meaningfully examined together (a "harmony"), or when they cannot (a "clash"). However, even after excluding clashes, the list of harmonies left could be enormous for exhaustive exploration. Hence, there is a need to reduce the search even further by including only those harmonies which are informative enough. @inference and @Majumder2013-hb present some methods to quantify the strength of pattern and noise through visual inference, similar to numerical testing. But this is an evolving field as human cognition (which act as the statistical tests in visual inference) might vary across humans even for the same plots. In this paper, we build a new distance measure which could be used to detect automatically detect significant periodic patterns.


Our contributions in this paper are:

 * We introduce a new distance measure for detecting periodic interactions. This induces data reduction which allows for identification of patterns, if any, in the time series data.

 * We show that the distance metric could be used to rank the periodic patterns based on how well they capture the variation in the measured variable as they have been normalized for different number of comparisons.

 * We device a framework for choosing a threshold, which will result in detection of only significantly interesting periodic patterns in the time series data.


The article is organized as follows. Section \ref{sec:computation-wpd} introduces a new distance measure, discusses the reasoning behind choosing such a measure and presents some results to study the behavior of the measure. Section \ref{sec:norm-wpd} describes a methodology to normalize the distance measure so that it can qualify as a measure that can be compared across different comparisons and datasets. Section \ref{sec:rank-wpd} discusses how to choose a threshold to select only significant harmonies. Section \ref{sec:application-wpd} presents an application to a residential smart meter data in Melbourne to show how this distance measure acts as a way to automatically detect periodic patterns in time series.


<!-- Also, ranking the remaining harmony pairs based on how well they capture the variation in the measured variable could be potentially useful.   -->

<!-- Talk about Scagnostics: Tukey -->

# A distance measure for quantifying patterns in harmonies {#sec:computation-wpd}

We are interested in assessing the structure of the measured variable across bivariate cyclic granularities. We propose a measure called Weighted Maximum Pairwise Distances (wpd) to quantify the structure in such a design. The principle employed towards this goal is explained through a simple example explained in Figure \ref{fig:null4by2}. Each of these figures have the same panel design with $2$ x-axis categories and $3$ facet levels. Figure \ref{fig:null4by2}a has all x categories drawn from N(5, 10) distribution for each facet. It is not an interesting display particularly, as distributions do not vary across x-axis or facet categories. Figure \ref{fig:null4by2}b has x categories drawn from the same distribution within a facet but the mean has been incremented by $5$ units for every consecutive facets. Figure \ref{fig:null4by2}c exhibits an exact opposite situation where distribution between the x-axis categories within each facet is different but they are same across facets. For this situation, mean of only the x-axis categories are increased by $5$ units for each consecutive category. Figure \ref{fig:null4by2}d takes a step further by varying the distribution across both facet and x-axis categories. If the displays are to be ranked in order of importance from minimum to maximum, then an obvious choice would be placing a followed by b, c and then d. It might be argued that it is not clear if b should precede or succeed c in the ranking. Gestalt theory suggests that when items are placed in close proximity, people assume that they are in the same group because they are close to one another and apart from other groups. Hence, displays that capture more variation within different categories in the same group would be important to bring out different patterns of the data. With this principle in mind, display b is considered less informative as compared to display c. Hence, with reference to the graphical design in @Gupta2020-vo, therefore the idea would be to rate a harmony pair higher if the variation between different levels of the x-axis variable is higher on an average across all levels of the facet variables. 

Intuitively, while finding a structure or measuring the strength of patterns in Figure \ref{fig:null4by2}, it makes sense to look for within-group and between-group variations. Larger variation would imply stronger patterns, whereas small variation would imply the underlying structure is not changing within or between group.
Thus, a distance measure aimed to capture this structure should ideally estimate these within-group and between-group variations. One of the potential ways to do this is to measure the distances between distributions of the continuous random variable measured within and between groups, weigh them basis if they are within or between groups and then take the maximum of those distances as an estimate of the maximum variation in the structure. This section starts with possible ways of characterizing distributions and computing distances between them and then describe in details how the measure $wpd$ is defined. This is similar to @ieee-irish where the authors compute the Jensen Shannon distance between two density estimates by computing percentiles and stresses the advantages to working with percentiles rather than the raw data directly in case of missing observations. Working with quantiles also ensure that unsynchronized time series could be handled.

<!-- To elaborate further, look at the examples in Figure \ref{}, where Figure \ref{}a represents the panel design with distribution of each x categories drawn from N(5, 10) distribution. It could be observed that the graph is not particularly interesting, as there is no significant change in distribution between x-axis levels or facets. Figure \ref{}b represents the same panel design with no difference in distribution of x-axis categories within a facet, but different distribution of x-axis categories for different facets. For example, if there are 4 facet levels and 2 x-axis levels, data is generated in the way as described in Table \ref{}. Figure \ref{}b exhibits an exact opposite situation where the x-axis within facets are different but not across facets. -->
<!-- Figure \ref{}d takes it further by varying the distribution across both facet and x-axis categories. -->



```{r null4by2,fig.cap = "A graphical display with two categories mapped to x-axis and 3 categories mapped to facets with the distribution of a continuous random variable plotted on the y-axis. Display a is not interesting as the distribution of the variable does not depend on x or facet categories. Display b and c are more interesting than a since there is a change in distribution either across facets (b) or x-axis (c). Display d is most interesting in terms of displaying the strongest pattern as distribution of the variable changes across both facet and x-axis variable."}

sim_varall_normal <- function(nx, nfacet, mean, sd, w) {
  dist_normal((mean + seq(0,
    (nx *
      nfacet - 1),
    by = 1
  ) * w), sd)
}
sim_panel_varall <- sim_panel(
  nx = 2, nfacet = 3,
  ntimes = 500,
  sim_dist = sim_varall_normal(2, 3, 5, 10, 5)
) %>% unnest(data)

p_varall <- sim_panel_varall %>%
  rename("facet level" = "id_facet" ) %>% 
  ggplot(aes(x = as.factor(id_x), y = sim_data)) + 
  facet_wrap(~`facet level`,labeller = "label_both") + 
  geom_boxplot() +
  ggtitle("") +
  xlab("x level") +
  ylab("")


sim_varx_normal <- function(nx, nfacet, mean, sd, w) {
   rep(dist_normal((mean + seq(0, nx - 1, by = 1) * w), sd), nfacet)
 }

sim_panel_varx <- sim_panel(
   nx = 2, nfacet = 3,
   ntimes = 500,
   sim_dist = sim_varx_normal(2, 3, 5, 10, 5)
 ) %>% unnest(data)


p_varx <- sim_panel_varx %>%
  rename("facet level" = "id_facet" ) %>% 
  ggplot(aes(x = as.factor(id_x), y = sim_data)) + 
  facet_wrap(~`facet level`,labeller = "label_both") + 
  ggtitle("") +
  geom_boxplot() +
  xlab("x level") +
  ylab("simulated response")



sim_varf_normal <- function(nx, nfacet, mean, sd, w) {
  rep(dist_normal((mean + seq(0, nfacet - 1, by = 1) * w), sd), each = nx)
}

sim_panel_varf <- sim_panel(
  nx = 2, nfacet = 3,
  ntimes = 500,
  sim_dist = sim_varf_normal(2, 3, 5, 10, 5)
) %>% unnest(data)


p_varf <- sim_panel_varf %>%
  rename("facet level" = "id_facet" ) %>% 
  ggplot(aes(x = as.factor(id_x), y = sim_data)) +
  facet_wrap(~`facet level`,labeller = "label_both") + 
  geom_boxplot() +
  ggtitle("") +
  xlab("x level") +
  ylab("")



sim_panel_null <- sim_panel(
   nx = 2,
   nfacet = 3,
   ntimes = 500,
   sim_dist = distributional
   ::dist_normal(5, 10)
) %>% unnest(c(data))

set.seed(9999)


p_null <- sim_panel_null %>%
  rename("facet level" = "id_facet" ) %>% 
  ggplot(aes(x = as.factor(id_x), y = sim_data)) +
  facet_wrap(~`facet level`,labeller = "label_both") +
  geom_boxplot() +
  ggtitle("") +
  xlab("x level") +
  ylab("simulated response")


ggpubr::ggarrange(p_null, p_varf,  p_varx, p_varall, nrow = 2, ncol = 2,
                  common.legend = TRUE,
                  labels = c("a", "b", "c", "d"))

```


<!-- in the same group would be important to bring out different patterns of the data. -->

## Notations

Consider two cyclic granularities $A$ and $B$, such that $A = \{ a_j: j = 1, 2, \dots, J\}$ and $B = \{ b_k: k = 1, 2, \dots, K\}$ with $A$ placed across x-axis and $B$ across facets. Let $v = \{v_t: t = 0, 1, 2, \dots, T-1\}$ be a continuous variable observed across $T$ time points. Let the four elementary designs as described in Figure \ref{fig:null4by2} be $D_{null}$ where there is no difference in distribution of $v$ for $A$ or $B$, $D_{var_f}$ denotes the set of designs where there is difference in distribution of $v$ for $B$ and not for $A$. Similarly, $D_{var_x}$ denotes the set of designs where difference is observed only across A. Finally, $D_{var_{all}}$ denotes those designs for which difference is observed across both $A$ and $B$.


```{r}
d1 <- tibble (variable = c("nx", "nfacet", "$N_C$", "$H_{N_C}$", "$lambda$", "$omega$", "$wpd$","$nperm$", "$nsim$"),  description = c("number of x categories", "number of facet categories","number of cyclic granularities", "set of harmonies", "tuning parameter", "mean increment", "raw distance measure", "number of permutations for threshold/normalization", "number of simulations"))
        
        
d2 <- tibble(variable =  c("$wpd_{norm}$", 
                           "$wpd_{threshold}$",
                           "$D_{null}$",
                           "$D_{var_f}$", 
                           "$D_{var_x}$",
                           "$D_{var_{all}}$"), 
             description = c("normalized distance measure",
                             "threshold for significance",
                             "null design",
                             "design with varying facets only", 
                             "design with varying x only", 
                             "design with varying both facets and x"))


d <- bind_rows(d1, d2)
knitr::kable(d, format = "markdown", escape = FALSE, caption = "Nomenclature table")

```


## Characterising distributions

Multiple observations of $v$ correspond to the subset  $v_{jk} = \{s: A(s) = j, B(s) = k\}$. The number of observations and the structure might vary widely across subsets due to the structure of the calendar, missing observations or uneven locations of events in the time domain. Each $v_{jk}$'s $\forall j\in \{1, 2, \dots, J\}, k\in \{1, 2, \dots, K\}\}$ are assumed to be drawn from a continuous probability distribution and have certain characteristics. Often shape, central tendency, and variability are the common characteristics used to describe a distribution. Mean, median or mode are generally used to describe the center of the distribution, while range, standard deviation, quantiles, standard errors and confidence intervals are used to describe variability. Quantiles are chosen as a way to characterize distributions in this paper.


The quantile of a distribution with probability $p$ is defined as $Q(p)=F^{-1}(p) = inf\{x: F(x) >p\}$, $0<p< 1$ where $F(x)$ is the distribution function. There are two broad approaches to quantile estimation, viz, parametric and non-parametric. The benefit of using a non-parametric estimator is that there are less rigid assumptions made about the nature of the underlying distribution of the data. Sample quantiles could be used for estimating population quantiles in a non-parametric setup. @Hyndman1996-ty describes the many ways of defining sample quantiles and recommends the use of median-unbiased estimator because of _desirable properties of a quantile estimator and can be defined independently of the underlying distribution._. The `stats::quantile()` function in @R-language could be used for practical implementation where type = 8 refers to the algorithm corresponding to the median-unbiased estimator. The default quantile chosen in this paper is percentiles computed for $p = {0.01, 0.02, \dots, 0.99}$, where for example, the $99^{th}$ percentile would be the value corresponding to $p=0.99$ and hence 99% of the observations would lie below that.


## Distance between distributions

<!-- One of the most important class of divergence is the f-divergence and includes measures like Kullback-Leibler divergence, Hellinger distance etc. The continuous version of f -divergence is given by -->
<!-- $$D_f(P||Q) := \int q(x)f(\frac{p(x)}{q(x)})$$, where -->
<!-- $f : [0,\infty) \rightarrow R \cup \{\infty\}$ is a continuous convex function, and $f(1) = 0$.  -->
  
The most common divergence measure between distributions is the Kullback-Leibler (KL) divergence [@Kullback1951-jy] introduced by Solomon Kullback and Richard Leibler in 1951. The KL divergence denoted by $D(q_1||q_2)$ is a non-symmetric measure of the difference between two probability distributions $q_1$ and $q_2$ and is interpreted as the amount of information lost when $q_2$ is used to approximate $q_1$. Although the KL divergence measures the “distance” between two distributions, it is not a distance measure since it is not symmetric and does not satisfy the triangle inequality. The Jensen-Shannon divergence [@Menendez1997-in] based on the Kullback-Leibler divergence is symmetric and it always has a finite value. The square root of the Jensen-Shannon divergence is a metric, often referred to as Jensen-Shannon distance. Other common measures of distance between distributions are Hellinger distance, total variation distance and Fisher information metric. 

In this paper, we propose to use the pairwise distances between the distributions of the measured variable through Jensen-Shannon distance (JSD), defined by,

$$JSD(q_1||q_2) = \frac{1}{2}D(q_1||M) + \frac{1}{2}D(q_2||M)$$
where $M = \frac{q_1+q_2}{2}$ and 
$D(q_1||q_2) := \int^\infty_{-\infty} q_1(x)f(\frac{q_1(x)}{q_2(x)})$ is the KL divergence between distributions $q_1$ and $q_2$. 

Furthermore, these distances are distributed as chi-squared with $m$ degrees of freedom (Menendez1997-in), if the continuous distribution is being discretized with $m$ discrete values. Taking sample percentiles to approximate the integral would mean taking $m = 99$. As the degrees of freedom $m$ get larger, the chi-square distribution approaches the normal distribution.



  <!-- Thus, by CLT, ${\chi^2}_{m} \tilde{} N(m, 2m)$, which would depend on the number of discretization used to approximate the continuous distribution.  -->
  <!-- Then $b_n = 1-1/n$ quantile of the normal distribution and $a_n = 1/[n*\phi(b_n)]$ where $\phi$ is the normal density function. $n$ is the number of pairwise comparisons being made. -->

<!-- The Jensen-Shanon distance between two probability distribution $p_1$ and $p_2$ is given by $$d = [D(p_1, r) + D(p_2, r)]/2 \quad where \quad r = (p_1 + p_2)/2$$ where, -->
<!-- $$D(p_1,p_2) = \int^{\infty}_{-\infty}p_1(x)log\frac{p_1(x)}{p_2(x)}\,dx$$ is the Kullback-Leibler divergence between $p_1$ and $p_2$.   -->

<!-- We call this measure of variation as  Median Maximum Pairwise Distances (MMPD). -->



<!-- #### Distribution of Jensen-Shannon distances -->

<!-- Jensen-Shannon distances (JSD) are distributed as chi-squared with $m$ df where we discretize the continuous distribution with $m$ discrete values. Taking sample percentiles to approximate the integral would mean taking $m = 99$. -->
<!-- With large $m$, chi-squared is asymptotically normal by the CLT. Thus, by CLT, ${\chi^2}_{m} \tilde{} N(m, 2m)$, which would depend on the number of discretization used to approximate the continuous distribution. Then $b_n = 1-1/n$ quantile of the normal distribution and $a_n = 1/[n*\phi(b_n)]$ where $\phi$ is the normal density function. $n$ is the number of pairwise comparisons being made. -->


## Computation

The distance measure $wpd$ between two cyclic granularities $A$ and $B$ is aimed to capture the strength of the structure by estimating the maximum within-group and between-group variations. Furthermore, the intended aim of $wpd$ is to capture differences in categories irrespective of the distribution from which the data is generated. Hence, as a pre-processing step,the raw data is normal quantile transformed (reference) so that the quantiles of the transformed data follows a standard normal distribution. The steps employed for computing the distance measure is summarized as follows:


<!-- The values of $wpd$ indeed depend upon the underlying distribution. This is also not desirable as the intended aim of $wpd$ is to capture differences in categories irrespective of the distribution from which the data is generated. The steps employed for computing the distance measure is summarized as follows: -->


<!-- Hence, differences in distributions of the measured variable are computed between all pairs of categories $(a_{j} b_{k}, a_{j'}b_{k'}): j = 1, 2, \dots, J\}, B = \{ b_k: k = 1, 2, \dots, K\}$. Percentiles of $v_{jk}$ and $v_{j'k'}$ are computed and stored in $q_{jk}$ and $q_{j'k'}$ respectively. Then the pairwise distances between pairs $(a_{j} b_{k}, a_{j'}b_{k'})$ be denoted as $d_{(jk, j'k')} = JSD(q_{jk}, q_{j'k'})$ is computed. Pairwise distances could be within-facets or between-facets. Figure \ref{fig:distance-explain} illustrates how the within-facet or between-facet distances are defined. Pairwise distances are within-facets ($d_{w}$) when $b_{k} = b_{k'}$, that is, between pairs of the form $(a_{j}b_{k}, a_{j'}b_{k})$ as shown in panel (3) of Figure \ref{fig:distance-explain}. If categories are ordered (like all temporal cyclic granularities), then only distances between pairs where $a_{j'} = (a_{j+1})$ are considered (panel (4)). Pairwise distances are between-facets ($d_{b}$) when they are considered between pairs of the form $(a_{j}b_{k}, a_{j}b_{k'})$. Number of between-facet distances would be $^{K}C_2*J$ and number of within-facet distances are $K*(J-1)$ (ordered) and $^{J}C_2*K$ (un-ordered). The pairwise distances $d_{(jk, j'k')}$ are transformed using a suitable tuning parameter ($0<\lambda<1$) depending on if they are $d_{b}$ or $d_{w}$ as follows: -->

```{r distance-explain, fig.cap = "Within and between-facet distances shown for two cyclic granularities A and B, where A is mapped to x-axis and B is mapped to facets. The dotted lines represent the distances between different categories. Panel 1) and 2) show the between-facet distances. Panel 3) and 4) are used to illustrate within-facet distances when categories are un-ordered or ordered respectively. When categories are ordered, distances should only be considered for consecutive x-axis categories. Between-facet distances are distances between different facet levels for the same x-axis category, for example, distances between {($a_1$,$b_1$) and ($a_1$, $b_2$)} or {($a_1$,$b_1$) and ($a_1$, $b_3$)}."}
knitr::include_graphics(here::here("paper/Figs/dist_explain.png"))
```

1. Perform NQT on the measured variable $v_t$ to obtain $v^*_t$.

2. Fix harmony pair $(A, B)$. 

<!-- The set of pairs of categories is of the form $(a_{j} b_{k}, a_{j'}b_{k'}): j = 1, 2, \dots, J\}, B = \{ b_k: k = 1, 2, \dots, K\}$. -->

3. Percentiles of $v^*_{jk}$ are computed and stored in $q_{jk}$. Repeat for all pairs of categories of the form $(a_{j} b_{k}, a_{j'}b_{k'}): \{a_j: j = 1, 2, \dots, J\}, B = \{ b_k: k = 1, 2, \dots, K\}$.

4. The pairwise distances between pairs $(a_{j} b_{k}, a_{j'}b_{k'})$  denoted by $d_{(jk, j'k')} = JSD(q_{jk}, q_{j'k'})$ is computed.

5. The pairwise distances $d_{(jk, j'k')}$ is transformed using a suitable tuning parameter ($0<\lambda<1$) depending on if they are within-facet($d_w$) or between-facets($d_b$) as follows:

\begin{equation}
d*_{(j,k), (j'k')} =
\begin{cases}
\lambda d_{(jk), (j'k')},& \text{if } d = d_w\\
(1-\lambda)  d_{(jk), (j'k')},              & \text{if } d = d_b\\
\end{cases}
\end{equation}

5. The wpd is then computed as $wpd = max_{j, j', k, k'}(d*_{(jk), (j'k')}) \forall j, j' \in \{1, 2, \dots, J\}, k, k' \in \{1, 2, \dots, K\}$.

6. Repeat Steps 2-5 for all harmony pairs in $H_{N_C}$.

\noindent Pairwise distances could be within-facets or between-facets. Figure \ref{fig:distance-explain} illustrates how the within-facet or between-facet distances are defined. Pairwise distances are within-facets when $b_{k} = b_{k'}$, that is, between pairs of the form $(a_{j}b_{k}, a_{j'}b_{k})$ as shown in panel (3) of Figure \ref{fig:distance-explain}. If categories are ordered (like all temporal cyclic granularities), then only distances between pairs where $a_{j'} = (a_{j+1})$ are considered (panel (4)). Pairwise distances are between-facets when they are considered between pairs of the form $(a_{j}b_{k}, a_{j}b_{k'})$. Number of between-facet distances would be $^{K}C_2*J$ and number of within-facet distances are $K*(J-1)$ (ordered) and $^{J}C_2*K$ (un-ordered). If the measure is intended to put more importance in pointing towards distributional differences between x categories, a $\lambda>0.5$ should be chosen. 



<!-- 5. Compute distribution of maximum distance ($M$) by shuffling the data $200$ times and finishing Steps1-4 in each case. Scale $M$ using the mean and sd of the distribution.

5. Use Steps 1-4 to compute maximum distance for $\forall k \in  \{1, 2, \ldots, K\}$.

6. Compute the distance measure MMPD_raw = median $(M_1, M_2, \dots, M_K)$.



<!-- All of these distances are then aggregated by taking the maximum from these distances to obtain $wpd$. -->


<!-- _j$ maps index set to a set $\{B_\ell \mid \ell =1,\dots,L\}$. Here, $A_k$ and $B_\ell$ are the levels/categories corresponding to $C_i$ and $C_j$ respectively. Let $S_{k\ell}$ be a subset of the index set such that for all $s \in S_{k\ell}$, $C_i(s) = A_k$ and $C_j(s) = B_\ell$. There are $KL$ such data subsets, one for each combination of levels ($A_k$, $B_\ell$). Moreover, consider that in the graphical space, $C_i$ is mapped to facets and $C_j$ is mapped to x-axis. -->


 <!-- old -->
<!-- Consider two cyclic granularities $C_i$ and $C_j$, such that $C_i$ maps index set to a set $\{A_k \mid k=1,\dots,K\}$ and $C_j$ maps index set to a set $\{B_\ell \mid \ell =1,\dots,L\}$. Here, $A_k$ and $B_\ell$ are the levels/categories corresponding to $C_i$ and $C_j$ respectively. Let $S_{k\ell}$ be a subset of the index set such that for all $s \in S_{k\ell}$, $C_i(s) = A_k$ and $C_j(s) = B_\ell$. There are $KL$ such data subsets, one for each combination of levels ($A_k$, $B_\ell$). Moreover, consider that in the graphical space, $C_i$ is mapped to facets and $C_j$ is mapped to x-axis.  -->


<!-- The algorithm employed for computing the distance measure is summarized as follows: -->

<!-- 1. Fix harmony pair $(C_i, C_j)$. -->

<!-- 2. Fix $k$. Then there are $L$ groups corresponding to level $A_k$ of $C_i$. -->

<!-- 3. Compute  $m = \binom{L}{2}$ pairwise distances between distributions of $L$ unordered levels and $m = L-1$ pairwise distances for $L$ ordered categories. -->

<!-- 4. Identify maximum within the $m$ computed distances. -->

<!-- <!-- 5. Compute distribution of maximum distance ($M$) by shuffling the data $200$ times and finishing Steps1-4 in each case. Scale $M$ using the mean and sd of the distribution.  --> 

<!-- 5. Use Steps 1-4 to compute maximum distance for $\forall k \in  \{1, 2, \ldots, K\}$. -->

<!-- 6. Compute the distance measure MMPD_raw = median $(M_1, M_2, \dots, M_K)$. -->


<!-- \begin{algorithm}[!thb] -->
<!-- 	\caption{Calculation for a raw distance measure between two cyclic granularities $A = \{ a_j: j = 1, 2, \dots, J\}$, $B = \{ b_k: k = 1, 2, \dots, K\}$ with $A$ placed across x-axis and $B$ across facets.}\label{alg:scoreopt}. -->
<!-- 	\begin{algorithmic}[1] -->
<!-- 		\Procedure{RawMMPD}{$A = \{ a_j: j = 1, 2, \dots, J\}$, $B = \{ b_k: k = 1, 2, \dots, K\}$, $v = \{ v_t: t = 1, 2, \dots, T\}$}. -->
<!-- 		\For{$k=1:K$, $j=1:J$} -->
<!-- 		\State Find distances between pairs of all possible combinations of categories $(a_jb_k,a_j'b_k')$ by computing JSD between quantiles of the measured variable $q(v)$ across these combinations. -->
<!-- 		\State $d \gets JSD(\tilde{q(v)_{a_{j}b_{k}}},\tilde{q(v)_{a_{j}'b_{k}'}})$ -->
<!-- 		\If {$b_k = b_k'$} -->
<!-- 		\State $d* \gets \lambda d$  \Comment{upweight within-facet distances} -->
<!-- 		\Else -->
<!-- 		\State $d* \gets 1/\lambda d $ \Comment{downweight across-facet distances} -->
<!-- 		\EndFor -->
<!-- 		\State Set the raw distance measure as $max(d*)$ where max is taken over all $j, j', k, k'$. -->

<!-- 		\EndProcedure -->
<!-- 	\end{algorithmic} -->
<!-- \end{algorithm} -->

<!-- A stronger measure "max" is chosen for aggregating x-axis categories compared to "median" for aggregating facet categories as the measure is intended to put more importance in pointing towards distributional differences between x-axis categories than for facet categories. -->

## Properties

Simulations were carried out to explore the behavior of $wpd$ under the following factors that could potentially impact the values of $wpd$: $nx$, $nfacet$, $\lambda$, $\omega$, $dist$ (normal/non-normal distributions with different location and scale), $ntimes$, and $designs$ and results are presented in two parts. The dependence of $wpd$ on $nx$ and $nfacet$ under $D_{null}$ is presented here, which lays the foundation for the next section. The rest of the results that discuss the relationship of the $wpd$ with other factors is presented in details in the Supplementary section of the paper. They show that the designs $D_{var_f}$ and $D_{var_x}$ intersect at $\lambda = 0.5$ and hence for up-weighing designs of the form $D_{var_x}$, $\lambda = 0.67$ has been considered for computation of $wpd$ in the rest of the paper. 

<!-- - $nx$ (number of levels of x-axis) -->
<!-- - $nfacet$ (number of levels of facets) -->
<!-- - $\lambda$ (tuning parameter) -->
<!-- - $\omega$ (increment in each panel design) -->
<!-- - $dist$ (normal/non-normal distributions with different location and scale) -->
<!-- - $n$ (sample size for each combination of categories) -->
<!-- - $designs$ ($D_{null}$, $D_{var_f}$, $D_{var_x}$ and $D_{var_{all}}$) -->

<!-- - $nsim$ (number of simulations) -->
<!-- - $nperm$ (number of permutations of data)   -->

### Simulation design{#sec:sim-study}

Observations are generated from a Gamma(2,1) distribution for each combination of $nx$ and $nfacet$ from the following sets: $nx = nfacet = \{2, 3, 5, 7, 14, 20, 31, 50\}$ to cover a wide range of levels from very low to moderately high. Each combination is being referred to as a _panel_. That is, data is being generated for each of the panels $\{nx = 2, nfacet = 2\}, \{nx = 2, nfacet = 3\}, \{nx = 2, nfacet = 5\},  \dots, \{nx = 50, nfacet = 31\}, \{nx = 50, nfacet = 50\}$. For each of the $64$ panels, $ntimes = 500$ observations are drawn for each combination of the categories. That is, if we consider the panel $\{nx = 2, nfacet = 2\}$, $500$ observations are generated for each of the combination of categories from the panel, namely, $\{(1, 1), (1, 2), (2, 1), (2, 2)\}$. The values
of $wpd$ is obtained for each of the panels. This design corresponds to $D_{null}$ as each combination of categories in a panel are drawn from the same distribution. Furthermore, the data is simulated for each of the panels $nsim=200$ times, so that the distribution of $wpd$ under $D_{null}$ could be observed. $wpd_{l, s}$ denotes the value of $wpd$ obtained for the $l^{th}$ panel and $s^{th}$ simulation.


### Results

Figure \ref{fig:raw} shows the distribution of $wpd$ plotted across different $nx$ and $nfacet$ categories. Since under $D_{null}$, there is no difference in distributions across different categories, we expect the distance measure $wpd$ to reflect that as well and have the same distribution across categories. But Figure \ref{fig:raw} shows that both the location and scale of the distributions change across panels. This is not desirable under $D_{null}$ as it would mean comparisons of $wpd$ values is not appropriate across different $nx$ and $nfacet$. Figure \ref{fig:quadratic} shows how the median of $wpd$ varies with the total number of distances $nx*nfacet$ for each panel. The median increases abruptly for lower values of $nx*nfacet$ and slowly for higher $nx*nfacet$. 

<!-- Furthermore, simulation results also show that the values of $wpd$ indeed depend upon the underlying distribution. This is also not desirable as the intended aim of $wpd$ is to capture differences in categories irrespective of the distribution from which the data is generated. -->


```{r raw, fig.cap = "Distribution of $wpd$ is plotted across different $nx$ and $nfacet$ categories under $D_{null}$. Both shape and scale of the distribution changes for different comparisons. This is not desirable since under null design, the distribution of the distance measure is not expected to capture any differences."}
G21 <- read_rds("simulations/raw/null_design_quantrans/data-agg/all_data_wpd_Gamma21.rds")

G21 %>% 
  ggplot(aes(x = value)) + 
  geom_density(fill = "blue") +
  facet_grid(nx~nfacet,
             labeller = "label_both") + 
  scale_x_continuous(breaks = scales::breaks_extended(3)) + 
  xlab("wpd")
```

```{r quadratic, fig.cap = "$wpd$ is plotted against $nx*nfacet$ (the maximum number of pairwise comparisons) and the blue line represents the median of the multiple values for each $nx*nfacet$. The median increases abruptly for lower values of $nx*nfacet$ and slowly for higher $nx*nfacet$. Thus, the measure will have higher values for higher levels in $nx$ or $nfacet$."}
G21 %>% 
  ggplot(aes(x=nx*nfacet, y = value)) +
  geom_point(alpha = 0.5, size = 0.5) + stat_summary(fun=median, geom="line", aes(group=1), color = "blue") + xlab("nx*nfacet") + ylab("wpd")

```

# Normalization for the number of comparisons {#sec:norm-wpd}

The distribution of $wpd$ is different for different levels of facets and x-axis levels. This is because the statistics maximum which is used to define $wpd$ is affected by the number of comparisons (resulting pairwise distances). The measure would have higher values if $A$ or $B$ has higher levels. However, we would ideally want a higher value of the measure only if there is a significant difference between distributions across facet or x-axis categories, and not because the number of categories $J$ or $K$ is high. Therefore, in order to compare $wpd$ across different combinations of facet and x-axis levels, we need to eliminate the impact of different number of comparisons and get a normalized measure. Henceforth, we call the normalized measure as $wpd_{norm}$. The measure $wpd_{norm}$ could potentially lead to comparison of the measure across different panels and also help distinguishing the interesting panels from a data set. We discuss two approaches for normalization, both of which are substantiated using simulations.

<!-- The nodes and the storage used are as follows: -->


## Methodology

The transformed ${wpd}$ which is normalized for the values of $nx$ and $nfacet$ is denoted by $wpd_{norm}$. Two approaches have been employed - the first one involves a permutation method to make the distribution of the transformed $wpd$ similar for different comparisons and the second one fits a model to represent the relationship between the two variables and defines $wpd_{norm}$ as the residual of the model.

<!-- ### Notations -->

<!-- Let $\{nx_{i}, i = 1, 2, \dots, nx\}$, $\{nfacet_{j}, j = 1, 2, \dots, nfacet\}$ be the set of x-axis and facet categories respectively. Each combination of $nx_{i}$ and $nfacet_{j}$is being referred to as a _panel_. Then the total number of panel is $nx*nfacet$. Let the total number of pairwise distances that could result in each panel be $\{z_k, k = 1, 2 , \dots, nx*nfacet\}$. Here, $\{z_1 = nx_1*nfacet_1\}$, $\{z_2 = nx_2*nfacet_2\}$ and $\{z_k = nx*nfacet\}$. Now, let $\{x_{k,l}, k = 1, 2, \dots, nx*nfacet, l = 1, 2, \dots, nsim\}$ denote the values of $wpd_{raw}$ obtained from the simulation study for $k^{th}$ panel in the $i^{th}$ simulation. Hence, for each of those $k$ panel, we have $nsim$ values of $wpd_{raw}$. -->


### Permutation approach

This method is somewhat similar in spirit to bootstrap (reference) or permutation (reference) tests, where the goal is to test the hypothesis that the groups under study have identical distributions. This method, in essence, accomplishes a different goal of making the location and scale of different groups (panels) same under $D_{null}$. The steps are as follows:

1. Compute $wpd$ for a harmony pair (A, B) for the original measured variable $v_t$ and store it in $wpd_{orig}$.

2. Consider a permutation of the original measured variable $v_{perm_1}$ and again compute $wpd$ for the permuted data. Store it in $wpd_{perm_1}$.

3. Repeat Step 2 for a large number ($nperm = 200$) of random permutations of the data yielding $nperm$ values : $wpd_{perm_1}, wpd_{perm_2}, \dots, wpd_{perm_{nperm}}$. Store the vector in $wpd_{perm}$.

4. Define $wpd_{perm} = \frac{(wpd_{orig} - \bar{wpd_{perm}})}{sd(wpd_{perm})}$, where $\bar{wpd_{perm}}$ and $sd(wpd_{perm})$ are the mean and standard deviation of $wpd_{perm}$ respectively.

<!-- Let $\{x_{k,l}, k = 1, 2, \dots, nx*nfacet, l = 1, 2, \dots, nsim\}$ denote the values of $wpd$ obtained from the simulation study for $k^{th}$ panel in the $i^{th}$ simulation. Hence, for each of those $k$ panel, we have $nsim$ values of $wpd$.combinations of $nx_i$ and $nfacet_j$. The normalized distances through permutation is computed as follows: $x^{perm}_{k} =  (x_{k} - mean_l(x_{k,l}))/sd_l(x_{k, l})$,  $x^{perm}_{k}$ is the value of the $wpd_{perm}$ for the $k^{th}$ panel-->


Standardizing  $wpd$ in the permutation approach ensures that the distribution of $wpd_{perm}$ has the same $mean = 0$ and $sd = 1$ across all comparisons under $D_{null}$. While this works successfully to make the location and scale similar across different $nx$ and $nfacet$ (as seen in Figure \ref{fig:norm}), it is computationally heavy and time consuming, and hence less user friendly when being actually used in practice. Hence, we propose another approach to normalization which is more approximate than exact but still has the similar accuracy when compared to the permutation approach.


```{r norm, fig.cap = "Distribution of $wpd_{perm}$ is plotted across different $nx$ and $nfacet$ categories. Both shape and scale of the distributions are now similar for different panels under the null design."}
G21_norm <- read_rds(here("simulations/norm/null_design_quantrans_nperm/data-agg/all_data_wpd_Gamma21.rds"))

G21_norm %>% 
  ggplot(aes(x = value)) + 
  geom_density(fill = "blue") +
  facet_grid(nx~nfacet,
             labeller = "label_both") + 
  scale_x_continuous(breaks = scales::breaks_extended(3)) + 
  xlab("wpd normalised using permutation approach")
```



### Modelling approach

_Linear model_  

A linear model is fitted to model the relationship between $wpd$ and $nx*nfacet$. The model is of the form $$y_l = a+b*log(z_l) + e_l$$, where, $y_l = median_m(wpd_{l, m})$, $z_l$ is the $l^{th}$ panel and $e_l$ are idiosyncratic errors, with parameters $a$ and $b$ being estimated from the data generated through the simulation study described in Section \ref{sec:sim-study}. The estimates and other model summary is given in Table \ref{tab:linear-model}.

```{r linear-model, message=FALSE}
G21 <- read_rds(here("simulations/raw/null_design_quantrans/data-agg/all_data_wpd_N01.rds"))

G21_median <- G21 %>% 
  group_by(nx*nfacet) %>% 
  summarise(actual = median(value))


# fit model median to log(nx*nfacet)
fit_lm2 <- lm(actual ~ log(`nx * nfacet`) , data = G21_median)

broom::tidy(fit_lm2) %>% kable(caption = "Results of linear model to capture the relationship between wpd and number of comparisons.")
```


<!-- # ```{r} -->
<!-- # results3 <- broom::tidy(fit_lm2) -->
<!-- # library(ggfortify) -->
<!-- # autoplot(fit_lm2, which = 1:6) + theme_minimal() -->
<!-- # ``` -->

The idea is to find a transformation on $wpd$ which would remove the effect of $nx*nfacet$ on $wpd$ and thus is defined as the residuals:
$y^* =  y - \hat a -  \hat b*log(z)$, where 
$y^*$ is the $wpd_{linear}$, 
$\hat a$ and $\hat b$ are the estimated values of the parameter $a$ and $b$ and $z = nx*nfacet$.

Defining $wpd_{linear}$ in this way forces the mean to be zero and variability to be uniform across the median $wpd$ values as could be seen in Figure \ref{fig:resi-linear}.  
<!-- But, the original distribution will still have some dissimilarities in shape and location specially for small values of $nx$ and $nfacet$ as could be seen in \ref{fig:} -->

```{r resi-linear, fig.cap = "$wpd_{norm}^{linear}$ which are defined as the residuals of the linear model have mean zero and are homogenous. Further, by design they are independent of the $nx*nfacet$ and hence could be a potential candidate for $wpd_norm$."}

intercept <- fit_lm2$coefficients[1]
slope <- fit_lm2$coefficients[2]

G21 %>% 
  ggplot(aes(x=log(nx*nfacet), y = (value - slope*log(nx*nfacet)))) +
  geom_point(alpha = 0.5, size = 0.5) + stat_summary(fun=mean, geom="line", aes(group=1), color = "blue") + 
  ylab("wpd normalised using linear modelling approach")
```


<!-- Let ${x_p, p = 1, 2, \dots, npanel}$ denote the total number of distances $nx*nfacet$ obtained in each panel. So for each $nx*nfacet$, we have $nsim = 200$ values of ${y_i}$. -->



<!-- from $nx_{i}$ x-categories and $nfacet_{j}$ facet-categories is $nx_{i}*nfacet_{j}$ and  -->

_Generalized linear model_  

In the linear model approach, $wpd\in R$ was assumed, whereas, $wpd$ is a  Jensen-Shannon Distance (JSD) and lies between 0 and 1 (reference). Furthermore, JSD follows a Chi-square distribution, which is a special case of Gamma distribution. Therefore, a generalized linear model could be fitted instead of a linear model to allow for the response variable to follow a Gamma distribution. The inverse link is used when we know that the mean response is bounded, which is applicable in our case since $0 \leq wpd\leq 1$.

We fit a Gamma generalized linear model with the inverse link which is of the form:  
$$y_l = a+b*log(z_l) + e_l$$, where $y_l = median_m(wpd_{l, m})$, $z_l$ is the $l^{th}$ panel and $e_l$ are idiosyncratic errors. Let $E(y) = \mu$ and $a + b*log(z) = g(\mu)$ where $g$ is the link function. Then $g(\mu)=  1/\mu$ and $\hat \mu  = 1/(\hat a + \hat b log(z))$. The residuals from this model $(y-\hat y) = (y-1/(\hat a + \hat b log(z)))$ would be expected to have no dependency on $z$. Thus, $wpd_{glm}$ is chosen as the residuals from this model and is defined as:
$wpd_{glm} = wpd - 1/(\hat a + \hat b*log(nx*nfacet))$.

<!-- Please note: -->

<!-- - subtracting the intercept brings the location of the transformed variable to 0 for either case -->
<!-- - heterogeneity is higher for this case after normalization as compared to the earlier one. -->

```{r wpd-glm-horizontal, echo = FALSE, message=FALSE, fig.caption = "$wpd_{glm}$ is plotted against log(nx*nfacet) and this transformation causes the median of $wpd_{glm}$ to have no relationship with log(nx*nfacet)"}

G21 <- read_rds(here("simulations/raw/null_design_quantrans/data-agg/all_data_wpd_N01.rds"))

G21_median <- G21 %>% 
  group_by(nx*nfacet) %>% 
  summarise(actual = median(value))



glm_fit <- glm(actual ~ log(`nx * nfacet`),
              family = Gamma(link = "inverse"),
              data = G21_median)

intercept <- glm_fit$coefficients[1]
slope <- glm_fit$coefficients[2]


G21_sd  = G21 %>% 
    mutate(wpd_glm =  (value - (1/(intercept + slope*log(nx*nfacet)
    )
    )
    ))

scale_fac <- 1/G21_sd$wpd_glm %>% sd()


# checking the fit of the residuals from glm fit

# fitted_glm <- fitted(glm_fit, type = "response")
# residuals <- residuals.glm(glm_fit, type = "response")
# hist(residuals)


#h = augment(glm_fit)
# ggplot(h) +
#   geom_histogram(aes(x = .resid))


# residual <- G21_median$actual  - fitted_glm
# 
# hist(residual)



G21 %>% 
  ggplot(aes(x=log(nx*nfacet),
             y = (value - (1/(intercept + slope*log(nx*nfacet)
                    )
                    )
             )
  )
  ) +
  geom_point(alpha = 0.5, size = 0.5) + stat_summary(fun=mean, geom="line", aes(group=1), color = "blue") + 
  ylab("wpd_glm = wpd - 1/(a  + b*log(nx*nfacet))")
```

```{r glm-tab}
broom::tidy(glm_fit) %>% kable(caption = "Results of generalised linear model to capture the relationship between wpd and number of comparisons.")
```


```{r wpd-glm-dist, fig.cap = " The distribution of $wpd_{glm}$ is plotted. The distributions are more similar across higher nx and nfacet and dissimilar for fewer nc and nfacets."}


G21_glm <- G21 %>% 
  mutate(wpd_glm =  (value - (1/(intercept + slope*log(nx*nfacet)
                    )
                    )
             ),
         wpd_glm_scaled = ((wpd_glm*320)))

#G21_glm$wpd_glm_scaled %>% sd()


G21_glm %>% 
  ggplot() +
  geom_density(aes(x = wpd_glm), 
                   fill = "blue") +
  facet_grid(nx~nfacet,
             labeller = "label_both") +
  theme(legend.position = "bottom") 
```


## Combining normalizing approaches

We see that the transformation through the modeling approach leads to very similar distribution across high $nx$ and $nfacet$ (higher than $5$) and not so much for lower $nx$ and $nfacet$. Hence, the computational load of permutation approach could be alleviated by using the modeling approach for the higher $nx$ and $nfacet$, however, it is important that we use the permutation approach for lower $nx$ and $nfacet$. It is difficult to compare and use the transformed measure from both of these approaches alternatively without bringing them to the same scale. The transformed variables from the two approaches have to be brought to the same scale so that for smaller categories, permutation approach is used and for larger categories, modeling approach is used. 

<!-- These could be done through the following: -->

<!--  - Making the range of both the variables same by using min-max scaling method. In practice, however, we would only have one value of $wpd$ which we need to transform using the modeling approach. Hence, min-max scaling approach could not be used here. -->

<!--  - Standardizing the variables and expressing scores at standard deviation units. Again in practice, however, we would only have one value of $wpd_{raw}$ which we need to transform using the modeling approach. Hence, standardizing scores could not be used here as we do not have the mean and standard deviation of a series while using transformation using modeling. -->

<!-- - Make the location and scale of both the approaches similar so that they could be compared. Please note that the range of values could be different in this case, however location and scale are brought to same levels.) -->

The measure $wpd_{glm}$ has a $\hat \mu_{glm} = 0$ and $\hat sd_{glm} = 0.003$ whereas the measure $wpd_{perm}$ which is a z-score, has an expected normal distribution with  $\hat \mu_{perm} = 0$ and $\hat sd_{perm} = 1$. To bring them to the same scale, we have defined $wpd_{glm-scaled} = wpd_{glm}*\frac{\hat sd_{perm}}{\hat sd_{glm}}$, which changes the scale of $wpd_{glm}$ without changing the location. The measure $wpd_{glm-scaled}$ seems to roughly follow a normal distribution except in the tails as could be seen in Figure \ref{fig:hist-qq} and the very method of permutation approach ensures that $wpd_{perm}$ is also normally distributed. Further, they are brought to similar scale and location and hence could be compared or used interchangeably for different comparisons based on their performance.

```{r hist-qq, fig.cap = "In panel a, the histogram of $wpd_{glm-scaled}$ is plotted. In part b, the QQ plot is shown with the theoretical quantiles on the x-axis and $wpd_{glm-scaled}$ quantiles on the y-axis. The distribution looks symmetric and looks like normal except in the tails."}
hist <- ggplot(G21_glm) +
  geom_histogram(aes(x = wpd_glm_scaled)) +
  ggtitle("a") + xlab("transformed wpd using glm approach")


p <- ggplot(G21_glm, aes(sample = wpd_glm_scaled))
qqplot <-  p + geom_qq() + geom_qq_line() + coord_fixed(ratio = 4/5) +
  ggtitle("b")

# G21_glm <- G21 %>% 
#     mutate(wpd_glm =  (value - (1/(intercept + slope*log(nx*nfacet)
#     )
#     )
#     ),
#     wpd_glm_scaled = 300*(wpd_glm))

 # G21_glm$wpd_glm_scaled %>% 
 #   range()
hist + qqplot
```  

```{r dist-new-same-scale-link, fig.cap = "$wpd_perm$ and $wpd_{glm-scaled}$ are plotted together on the same scale. They also have the same location and hence the values from these two approaches could be compared across panels. $wpd_{glm-scaled}$ would be used to normalise $wpd_{raw}$ for higher $nx$ and $nfacet$ and $wpd_{glm-scaled}$ would be used for smaller levels to alleviate the problem of computational time."}

G21_permutation <- read_rds(here("simulations/norm/null_design_quantrans_nperm/data-agg/all_data_wpd_N01.rds")) %>%
  rename("wpd_permutation" = "value")


# G21_model_data <- G21 %>%
#   mutate(model =
#            ((1/value)
#                   - intercept -
#                     slope*log(nx*nfacet))/slope) %>%
#   mutate(model_trans =
#            (model - mean(model))/sd(model))

# G21_model_data$model %>% summary()

G21_all_data <- G21_permutation %>% 
  # left_join(G21_lm, by = c("nx", "nfacet", "perm_id")) %>% 
  left_join(G21_glm, by = c("nx", "nfacet", "perm_id")) %>% 
  pivot_longer(cols = c(3, 7),
               names_to = "type_estimate",
               values_to = "value_estimate")
G21_all_data$type_estimate = factor(G21_all_data$type_estimate , levels = c( "wpd_permutation", "wpd_glm_scaled"))


G21_all_data %>% 
  filter(type_estimate %in% c("wpd_glm_scaled", "wpd_permutation")) %>% 
  ggplot() +
  geom_density(aes(x = value_estimate, 
                   fill = type_estimate), alpha = 0.5) +
  facet_grid(nx~nfacet,
             labeller = "label_both") +
  theme(legend.position = "bottom") +
  scale_fill_manual(values = c( "#D55E00", "#0072B2")) +
  xlab("wpd_norm2") +
  scale_x_continuous(breaks = c(-5, -3, 0, 3, 5))

# G21_all_data %>% 
#   filter(type_estimate %in% c("wpd_permutation", "wpd_glm")) %>% 
#   ggplot() +
#   geom_density(aes(x = value_estimate, 
#                    fill = type_estimate), 
#                alpha = 0.7) +
#   facet_grid(nx~nfacet,
#              labeller = "label_both") +
#   theme(legend.position = "bottom") +
#   scale_fill_manual(values = c("#CC79A7", "#0072B2")) +
#   xlab("wpd_norm2")

# are scales same
# 
# G21_model_data %>%
#   group_by(nx, nfacet) %>% 
#   summarize(sd_model = sd(model))
#   
# G21_all_data %>%
#   filter(type_estimate == c("perm_trans")) %>% 
#   select(value_estimate) %>% 
#   range()
# 
# G21_all_data %>%
#   filter(type_estimate == c("wpd_glm_scaled")) %>% 
#   select(value_estimate) %>% 
#   range()
```

Thus, the $wpd_{norm}$ is defined as follows:

\begin{equation}
wpd_{norm} =
\begin{cases}
wpd_{perm},& \text{if } J, K <=5\\
wpd_{glm-scaled}             & \text{otherwise}\\
\end{cases}
\end{equation}

## Properties

This section reports the results of a simulation study that was carried out to evaluate the behavior of $wpd_{norm}$ under different designs and other potential factors. The behavior of $wpd_{norm}$ is explored in designs where there is in fact difference in distribution between facet categories ($D_{var_f}$) or across x-categories ($D_{var_x}$) or both ($D_{var_{all}}$). Using $\omega =  \{1, 2, \dots, 10\}$ and $\lambda = seq(from  = 0.1, to = 0.9, by = 0.05)$, observations are drawn from a N(0,1) distribution for each combination of $nx$ and $nfacet$ from the following sets: $nx = nfacet =  \{2, 3, 5, 7, 14, 20, 31, 50\}$. $ntimes = 500$ is assumed for this setup as well. Furthermore, to generate different distributions across different combination of facet and x levels, the following method is deployed - suppose the distribution of the combination of first levels of $x$ and $facet$ category is $N(\mu,\sigma)$ and $\mu_{jk}$ denotes the mean of the combination $(a_jb_k)$, then $\mu_{j.} = \mu + j\omega$ (for design $D_{var_x}$) and $\mu_{.k} = \mu + k\omega$ (for design $D_{var_f}$). 

The tabulated values and graphical representations of the simulation results are provided in the Supplementary paper. The learning from the simulations are as follows: The values of $wpd_{norm}$ is least for $D_{null}$, followed by
$D_{var_f}$, $D_{var_x}$ and $D_{var_{all}}$. This is a desirable result since the measure $wpd_{norm}$ was designed such that this relationship holds. Furthermore, the distribution of the measure $wpd_{norm}$ does not change for different facet and x categories. The distribution of $wpd_{norm}$ looks similar with at least the mean and standard of the distributions being uniform across panels. This means $wpd_{norm}$ could be used to measure differences in distribution across panels. Also, note that since the data is processed using normal-quantile-transform, this measure is independent of the initial distribution of the underlying data and hence is also comparable across different data sets. This is valid for the case when sample size $ntimes$ for each combination of categories is at least 30 and  $nperm$ used for computing $wpd_{norm}$ is at least 100. More detailed results about the properties of $wpd_{norm}$ could be found in the Supplementary paper.

# Ranking and selecting significant harmonies {#sec:rank-wpd}

In this section, we provide a method to select important harmonies by eliminating all harmonies for which patterns are not significant by employing randomization test. Randomization tests (permutation tests) generates a random distribution by re-ordering our observed data and allow to test if the observed data is significantly different from any random distribution. Complete randomness in the measured variable indicates that the process follows a homogeneous underlying distribution over the whole time series, which essentially implies there is no interesting distinction across any different categories of the cyclic granularities. 


## Choosing a threshold

A randomization test involves calculating a test statistic, randomly shuffling the data and calculating the test statistic several times to obtain a distribution of the test statistic. But we will use this procedure to obtain a threshold such that harmony pairs with a $wpd_{norm}$ value higher than this threshold will only be considered significant. The process of choosing a threshold is described as follows:

<!-- to test if there is any interesting pattern captured by the harmonies, which essentially implies if $wpd_{norm}$ is significantly different from zero. The percentages of times the $wpd_{norm}$ obtained from the permuted data is greater than or equal to the observed $wpd_{norm}$ is the p-value. The randomization test is described as follows: -->

<!-- <!-- We can remove the harmonies for which no interesting patterns are observed through a randomization permutation method. --> 
<!-- Essentially, the assumption is that under the null hypothesis, there is no difference in categories between the pair of cyclic granularities in the chosen harmony. This method is based on the generation of randomly chosen reassignments (permutations) of the data across different cyclic granularities and the computation of  -->
<!-- $wpd_{norm}$ for each of these reassignments. -->


<!-- **Assumption:** random permutation of the data keeping the categories of the cyclic granularities constant. -->

- **Input:** All harmonies of the form $\{(A, B), A = \{ a_j: j = 1, 2, \dots, J\}, B = \{ b_k: k = 1, 2, \dots, K\}\}$  with $A$ placed across x-axis and $B$ across facets $\forall (A, B) \in N_C$.

- **Output:** Harmony pairs $(A, B)$ for which $wpd_{norm}$ is significant.

1. Fix harmony pair $(A, B)$.

2. Given the data; $\{v_t: t=0, 1, 2, \dots, T-1\}$, the $wpd_{norm}$ is computed and is represented by $wpd_{obs}$.

3. From the original sequence a random permutation is obtained: $\{v_t^*: t=0, 1, 2, \dots, T-1\}$.

4. $wpd_{norm}$ is computed for the permuted sequence of the data and is represented by $wpd_{perm_1}$.

5.  Steps (3) and (4) are repeated a large number
of times $M$ (M = 200).

6. For each permutation, one $wpd_{perm_i}$ is obtained. Define $wpd_{sample} = \{wpd_{perm_1}, wpd_{perm_2}, \dots, wpd_{perm_M}\}$.

7. Repeat Steps (1-6) for all harmony pairs $(A, B) \in H_{N_C}$ and store it in $wpd^{all}_{sample}$.

8. $99^{th}$ percentiles of $wpd^{all}_{sample}$ is computed and stored in $wpd_{threshold99}$

7. If $wpd_{obs_{A, B}} > wpd_{threshold99}$, harmony pair $(A, B)$ is selected with a 99% level of significance and otherwise rejected.

Similarly, a harmony pair $(A, B)$ is selected with a 95% and 90% level of significance if $wpd_{obs_{A, B}} > wpd_{threshold95}$ and $wpd_{obs_{A, B}} > wpd_{threshold90}$, where  $wpd_{threshold95}$ and $wpd_{threshold90}$ denote the $95^{th}$ and $90^{th}$ percentile of $wpd^{all}_{sample}$ respectively.

<!-- Complete randomness in the measured variable indicates that the process follows a homogeneous underlying distribution over the whole time series, which essentially implies there is no interesting distinction across any different categories of the cyclic granularities. We can remove the harmonies for which no interesting patterns are observed through a randomization permutation method. Essentially, the assumption is that under the null hypothesis, there is no difference in categories between the pair of cyclic granularities in the chosen harmony. This method is based on the generation of randomly chosen reassignments (permutations) of the data across different cyclic granularities and the computation of  -->
<!-- $wpd_{norm}$ for each of these reassignments. The percentages of times the theoretical distribution greater than or equal to the respective observed $wpd_{norm}$ values are calculated and are used to obtain the P value. The procedure for the permutation test is: -->

<!-- **Assumption:** random permutation of the data keeping the categories of the cyclic granularities constant. -->



<!-- 1. Given the data; $\{v_t: t=0, 1, 2, \dots, T-1\}$, the $wpd_{norm}$ is computed and is represented by $wpd_{obs}$. -->

<!-- 2. From the original sequence a random permutation is obtained: $\{v_t^*: t=0, 1, 2, \dots, T-1\}$. -->

<!-- 3. $wpd_{norm}$ is computed for the permuted sequence of the data and is represented by $wpd_{perm_1}$. -->

<!-- 4.  Steps (2) and (3) are repeated a large number -->
<!-- of times M (M = 200). -->

<!-- 5. For each permutation, one $wpd_{perm_i}$ is obtained. Define $wpd_{sample} = \{wpd_{perm_1}, wpd_{perm_2}, \dots, wpd_{perm_M}\}$. -->

<!-- 6. $95^{th}$ percentile of this $wpd_{sample}$ distribution is computed and stored in $wpd_{threshold}$. -->

<!-- 7. If $wpd_{obs}> wpd_{threshold}$, harmony pairs are accepted. Only one threshold for all harmony pairs. -->


## Simulation design

Observations are generated from a N(0,1) distribution for each combination of $nx$ and $nfacet$ from the following sets: $nx = \{3, 7, 14\}$ and $nfacet = \{2, 9, 10\}$. The panel $(3, 2), (7,9), (14, 10)$ are considered to have design $D_{null}$. The panels $(7, 2), (14, 9)$ have design of the form $D_{var_f}$. $(14, 2), (3, 10)$ have design of the form $D_{var_x}$ and the rest are under $D_{var_{null}}$. We generate only one data set for which all these designs were simulated and consider this as the original data set. We generate $200$ repetitions of this experiment with different seeds and compute the proportion of times a panel is rejected when it is under $D_{null}$. We also compute the proportion of times a panel is rejected when it actually belongs to a non-null design. The first proportion is desired to be as small as possible and a higher value of the later is expected. Also, these would constitute to be the estimated size and power of the test.

<!-- 14, 20, 31, 50\}$. For each of the $64$ panels, $ntimes = 500$ observations are drawn for each combination of the categories. The values of $wpd_{norm}$ is obtained for each of the panels for the designs $D_{null}$, $D_{var_x}$, $D_{var_f}$ and $D_{var_{all}}$. $wpd_{threshold}$ is computed from all of these panels together and the number of times a harmony pair $(A, B) \in H_{N_C}$ is selected when in fact it was of the design $D_{null}$ is noted. This entire process is repeated for several null data sets to see the number of times any harmony pair $(A, B) \in H_{N_C}$ is selected under null. -->


### Results

The results for this section is WIP and to be included in details in the Supplementary paper. Also, Figure \ref{fig:plot-all-new2} presents the results of $wpd_{norm}$ from the illustrative designs in Section \ref{sec:computation-wpd}. As expected, the value of $wpd_{norm}$ under null design is the least (a), followed by (b, c and d). Moreover, note the the relative difference in $wpd_{norm}$ values as we move from  a to d, which aligns with the idea of $wpd_{norm}$, since the differences between x categories are up-weighed by design.


```{r varall-new}

sim_varall_normal <- function(nx, nfacet, mean, sd, w) {
  dist_normal((mean + seq(0,
    (nx *
      nfacet - 1),
    by = 1
  ) * w), sd)
}
sim_panel_varall <- sim_panel(
  nx = 2, nfacet = 3,
  ntimes = 500,
  sim_dist = sim_varall_normal(2, 3, 0, 1, 10)
) %>% unnest(data)

set.seed(9999)
varall <- compute_pairwise_norm(sim_panel_varall, 
                           gran_x = "id_x",
                           gran_facet = "id_facet",
                           response = sim_data, 
                           nperm = 200
)

# plot
p_varall <- sim_panel_varall %>%
  ggplot(aes(x = as.factor(id_x), y = sim_data)) + facet_wrap(~id_facet) + geom_boxplot() +
  ggtitle(paste("d", round(varall, 2)))
```



```{r varx-new}
sim_varx_normal <- function(nx, nfacet, mean, sd, w) {
   rep(dist_normal((mean + seq(0, nx - 1, by = 1) * w), sd), nfacet)
 }

sim_panel_varx <- sim_panel(
   nx = 2, nfacet = 3,
   ntimes = 500,
   sim_dist = sim_varx_normal(2, 3, 0, 1, 10)
 ) %>% unnest(data)


set.seed(9999)
varx <- compute_pairwise_norm(sim_panel_varx, 
                           gran_x = "id_x",
                           gran_facet = "id_facet",
                           response = sim_data, 
                           nperm = 200
)

# plot
p_varx <- sim_panel_varx %>%
  ggplot(aes(x = as.factor(id_x), y = sim_data)) + facet_wrap(~id_facet) + geom_boxplot() +
    ggtitle(paste("c", round(varx, 2)))
```


```{r varf-new}
sim_varf_normal <- function(nx, nfacet, mean, sd, w) {
  rep(dist_normal((mean + seq(0, nfacet - 1, by = 1) * w), sd), each = nx)
}
sim_panel_varf <- sim_panel(
  nx = 2, nfacet = 3,
  ntimes = 500,
  sim_dist = sim_varf_normal(2, 3, 0, 1, 10)
) %>% unnest(data)


set.seed(9999)
varf <- compute_pairwise_norm(sim_panel_varf, 
                           gran_x = "id_x",
                           gran_facet = "id_facet",
                           response = sim_data, 
                           nperm = 200
)

p_varf <- sim_panel_varf %>%
  ggplot(aes(x = as.factor(id_x), y = sim_data)) + facet_wrap(~id_facet) + geom_boxplot() +
    ggtitle(paste("b", round(varf, 2)))
```


```{r null-new}
sim_panel_null <- sim_panel(
   nx = 2,
   nfacet = 3,
   ntimes = 500,
   sim_dist = distributional
   ::dist_normal(0, 1)
) %>% unnest(c(data))

set.seed(9999)

null <- compute_pairwise_norm(sim_panel_null, 
                           gran_x = "id_x",
                           gran_facet = "id_facet",
                           response = sim_data, 
                           nperm = 200
)

p_null <- sim_panel_null %>%
  ggplot(aes(x = as.factor(id_x), y = sim_data)) + facet_wrap(~id_facet) + geom_boxplot() +
    ggtitle(paste("a", round(null, 2)))
```

```{r plot-all-new2, fig.cap = "$wpd_{norm}$ values for the four illustrative designs are presented here. As expected, the value of $wpd_{norm}$ under null design is the least (a), followed by (b, c and d). Moreover, the relative difference is interesting as c is closer to d than b, since the differences between x categories are up-weighed by design."}
library(patchwork)
(p_null + p_varf)/(p_varx + p_varall)
```

## Simulation environment

Simulation studies were carried out to study the behavior of $wpd$, build the normalization method as well as compare and evaluate different normalization approaches. R version 4.0.1 (2020-06-06) is used with the platform: x86_64-apple-darwin17.0 (64-bit) running under: macOS Mojave 10.14.6 and MonaRCH, which is a next-generation HPC/HTC Cluster, designed from the ground up to address the computing needs of the Monash HPC community.

# Application to residential smart meter dataset {#sec:application-wpd}

The smart meter data set for eight households in Melbourne has been utilized to see the use of $wpd_{norm}$ proposed in the paper. The data has been cleaned to be a `tsibble` (@wang2019tsibble) containing half-hourly electricity consumption from Jul-2019 to Dec-2019 for each of the households, which is procured by them by downloading their data from the energy supplier/retailer. Demand data for these households are shown in a linear time scale in Figure \ref{fig:linear-scale-8}. It is evident from the range of the demand data that these households vary in consumption levels as well as in their temporal patterns. In the left panel of Figure \ref{fig:linear-scale-8} (a), the linear representation of the entire time period is shown, whereas in the right panel (b) a particular month is shown and furthermore a week has been highlighted to inspect if there is any daily or weekly periodic patterns in their behavior that is reflected when we zoom into the the linear representation of the time series. We have some additional demographics data in terms of the number of members in the household and presence of kids/elderly parents and their profession.


```{r}
demography <- tibble(id = c(1, 2, 3, 4, 5,6, 7, 8), 
       profession = c("academia", "mixed", "industry", "industry", "mixed", "mixed", "academia", "industry"),
       total_members = c(2, 3, 5, 5, 2, 2, 2, 6),
       kids = c("no", "no", "yes", "yes", "no", "no", "no", "yes"),
       old_parents = c("no", "yes", "yes", "yes", "no", "no", "no", "yes"),
       PhD_student = c("no", "yes", "no", "no", "yes", "yes", "yes", "no"))

demography %>% kable(format  = "markdown", caption = "Demographics of the eight households chosen for study")
```



We start the analysis by asking if the ranking of the harmonies make sense for the households, then compare households to get more insights of what these rankings imply and if they could be used to remove some non-interesting harmonies. Furthermore, we see if the display of the significant harmonies could be validated by zooming in the linear representation of the time series.

<!-- if we are benefiting by moving from a linear to cyclic representation of time to . -->

```{r linear-scale-8, fig.cap = "Electricity demand for eight households are shown in different facets from Jul-19 to Dec-19 in Fig a and it has been zoomed in for Sep-19 in Fig b, where a week in Sep-19 has been highlighted. From the scales of Fig a, it is apparent that they have different level of consumption but all of them have some periodic behavior in terms of regular peaks and troughs. It is not clear which all periodic patterns exist. In fig b, periodic pattern is zoomed in for a month and we can see weekly patterns for the entire period and daily pattern for the highlighted week."}
library(scales)
library(tidyquant)
library(gghighlight)
library(lubridate)

elec <- read_rds(here("paper/data/elec_all-8.rds")) %>% 
  dplyr::filter(date(reading_datetime) >= ymd("20190701"), date(reading_datetime) < ymd("20191231"), meter_id==1) %>% 
  select(-meter_id) %>% 
  rename("id" = "household_id",
         "date_time" = "reading_datetime") %>% 
  mutate(date = date(date_time))


elec_linear <- elec %>% 
  ggplot() +
  geom_line(aes(x = date_time, y = kwh),alpha = 0.7) +
  facet_wrap(~id, nrow = 8, labeller = "label_both",
             strip.position =  "right",
             scales = "free_y") + ggtitle("a")

elec_zoom <-  elec %>%
  as_tibble() %>% 
  filter(date >as.Date("2019-09-01") & date < (as.Date("2019-09-30"))) %>% 
  ggplot(aes(x=date_time, y = kwh)) +
  geom_line(size = 0.1, colour = "blue") +
  facet_wrap(~id, 
             scales = "free_y",
             ncol = 1,
             strip.position =  "right") +
  gghighlight(date > as.Date("2019-09-15") & date < (as.Date("2019-09-21")), unhighlighted_params = list(colour = "black")) + ggtitle("b")

elec_linear + elec_zoom

```



```{r raw-nqt-8, fig.height = 5, fig.cap="The raw density of the half-hourly demand for the eight households in Panel a. Panel b shows the normal-score-transform half-hourly demand for the same households which has resulted in more symmetric distribution of half-hourly demand."}
elec_raw <- elec %>% 
  ggplot() +
  geom_density(aes(x = kwh)) +
  facet_wrap(~id, scales = "free_y", ncol = 1) +
  xlab("demand (in kwh)") + ggtitle("a") +
  theme(
        strip.text = element_text(size = 10, margin = margin(b = 0, t = 0)))

elec_nqt <- elec %>% 
  dplyr::mutate(kwh_transformed = stats::qqnorm(kwh, plot.it = FALSE)$x) %>% 
  ggplot() +
  geom_density(aes(x = kwh_transformed))+
  facet_wrap(~id, scales = "free_y", ncol = 1) +
  xlab("NQT demand (in kwh)") + ggtitle("b") + theme(
        strip.text = element_text(size = 10, margin = margin(b = 0, t = 0)))

# hour_day_raw <- elec %>% 
#   tsibble::as_tsibble(index = date_time, key = id) %>% 
#     create_gran("hour_day") %>% 
#     ggplot() + 
#     ggridges::geom_density_ridges(aes(y=hour_day, x = kwh))
#   
# hour_day_trans <- elec %>% 
#   tsibble::as_tsibble(index = date_time, key = id) %>% 
#     create_gran("hour_day") %>% 
#   dplyr::mutate(kwh_transformed = stats::qqnorm(kwh, plot.it = FALSE)$x) %>% 
#   ggplot() +
#   ggridges::geom_density_ridges(aes(y=hour_day, x = kwh_transformed))

elec_raw + elec_nqt

```

_Choosing cyclic granularities of interest and removing clashes_

Let $v_{i, t}$ denote the electricity demand for $i^{th}$ household for time period $t$. The series $v_{i, t}$ is the linear granularity corresponding to half-hour since the interval of this data is 30 minutes. We consider coarser linear granularities like hour, day, week and month from the commonly used Gregorian calendar. Considering $4$ linear granularities hour, day, week, month in the hierarchy table, the number of cyclic granularities is $N_C = (4 * 3/2) = 6$. We obtain cyclic granularities namely “hour_day”, “hour_week”, “hour_month”, “day_ week”, “day_month” and “week_month”, read as “hour of the day”, etc. Further, we add cyclic granularity day-type( “wknd wday”) to capture weekend and weekday behavior. Thus, $7$ cyclic granularities are considered to be of interest. The set consisting of pairs of cyclic granularities ($C_{N_C}$) will have $7_{P_2}=42$ elements which could be analyzed for detecting possible periodicities. The set of possible harmonies $H_{N_C}$ from $C_{N_C}$ are chosen by removing clashes using procedures described in [@Gupta2020-vo]. Table \ref{tab:tab-rank-8}  shows $14$ harmony pairs that belong to $H_{N_C}$.

 <!-- The paper also shows how the number of harmonies could be identified among  by removing clashes. This section shows how we could refine the search further by only looking at significant harmonies. -->


_Selecting and Ranking harmonies for all households_ 

$v_{i, t}$ has a asymmetrical distribution as could be seen in \ref{fig:raw-nqt-8} and the Normal score transform has been applied to make it more symmetric. Let $v*_{i, t}$ denote the normal-quantile transformed electricity demand for $i^{th}$ household for time period $t$. Suppose $(A, B) \in H_{N_C}$ be a harmony pair where  $A = \{ a_j: j = 1, 2, \dots, J\}$ and $B = \{ b_k: k = 1, 2, \dots, K\}$ with $A$ placed across x-axis and $B$ across facets. Suppose $q^{i, p}_{A, j}$ denote the quantiles with probability $p$ for the of the $i^{th}$ household for $j^{th}$ category of the cyclic granularity $A$. Similarly, $q^{i, p}_{B, k}$ denotes the same for the $k^{th}$ category of the cyclic granularity $B$. Sample quantiles were computed at $p = 0.01, 0.02, \dots, 0.99$. Jensen-Shannon distances are computed between $q^{i, p}_{A, j}$ and $q^{i, p}_{B, k}$ for each ${j \in J, k \in K}$ to obtain within-facet and between-facet distances. A tuning parameter of $\lambda = 0.67$ has been considered to upweigh the within-facet distances and down-weigh the between facet distances and the maximum of them are obtained to compute $wpd$. It is further normalized using the approach described in Section \ref{sec:norm-wpd}. This entire process is repeated for all harmony pairs $\in H_{N_C}$ and for each households $i \in i = \{1, 2, \dots, 8\}$. The harmony pairs are then arranged in descending order and the important ones with significance level 1%, 5% and 10% are highlighted with `***`, `**` and `*` respectively. Table \ref{tab:tab-rank-8} shows the rank of the harmonies for different households
\ref{fig:dotplot-8} shows the heatmap for the eight households
with the value of $wpd_{norm}$ filled as colors.

_Validating rank of household id:1_

From table \ref{tab:tab-rank-8}, it could be seen that 
for household id:1, (hod, wdwnd) has been ranked higher than (wdwnd, hod), both of these being significant. Further, we see that (wom, wdwnd) has been ranked $5^{th}$ and tagged as an insignificant pair. Figure \ref{fig:gravitas-plot-8} is used to show
if this selection and ranking of harmony pairs makes sense for this household. Panel a) of  Figure \ref{fig:gravitas-plot-8} shows the distribution of energy demand with weekday/weekend as the x-axis and hour-of-day as the facets and helps to compare the weekend/weekday patterns for different hours of the day. It could be observed that the difference between weekend and weekday is the highest from 15 to 19 hours of the day. Panel b) shows the distribution of energy demand with the variables swapped and helps to compare the daily patterns within weekday and weekend. It could be observed that the daily pattern is similar for weekdays and weekends with a morning and evening peak. However,the difference between morning and evening peaks are higher for weekends. Since $wpd_{norm}$ is designed to put more weightage on within-facet differences, it makes sense that the pair (hod, wdwnd) has been ranked higher than (wdwnd, hod). Panel c) shows the distribution of energy demand with weekday/weekend as the x-axis and week-of-month as the facets. Although the differences might seem significant at first, with closer inspection it could be seen that the range of the demand is low in this case and hence the differences are not large enough to cross the threshold for significance.

```{r gravitas-plot-8, message = FALSE, warning=FALSE,fig.cap="Distribution of energy demand shown for household id 1 across hod in x-axis and wd-wnd in facets in a) and just the reverse in b). In c), distribution of energy demand for household id:4 shown across hod and wd-wnd. It can be seen that the differences in distributions are more apparent when viewed in a) as compared to b). It seems like there is more difference in the distributions of hod for b) compared to c). This also confers with the value of the normalised measure shown in Figure 11."}

id1_tsibble <- elec %>%
  filter(id== 1)

p1 <- id1_tsibble %>% 
   prob_plot("hour_day",
             "wknd_wday",
             response = "kwh",
             plot_type = "quantile",
             symmetric = FALSE,
             quantile_prob = c(0.25, 0.5, 0.75)) +
  ggtitle("a) hod vs wdwnd (Rank 1)") + 
  scale_colour_brewer(name = "", palette = "Set2") +
  theme(legend.position = "none",
        strip.text = element_text(size = 7, margin = margin()),)


p2 <- id1_tsibble %>%
   prob_plot("wknd_wday",
             "hour_day",
             response = "kwh",
             plot_type = "quantile",
             symmetric = FALSE,
             quantile_prob = c(0.25,0.5,0.75)) +
  ggtitle("b) wdwnd vs hod (Rank 3)") +
  scale_colour_brewer(name = "", palette = "Set2")   +
  theme(legend.position = "none",
        strip.text = element_text(size = 7, margin = margin())) 

p3 <- id1_tsibble %>%
   prob_plot("week_month",
             "wknd_wday",
             response = "kwh",
             plot_type = "quantile",
             symmetric = FALSE,
             quantile_prob = 
               c(0.25,0.5,0.75)) +
  scale_colour_brewer(name = "", palette = "Set2") +
  theme(
        strip.text = element_text(size = 10, margin = margin(b = 0, t = 0))) + theme(legend.position = "bottom",
        strip.text = element_text(size = 7, margin = margin())) +
  ggtitle("c) wom vs wdwnd (insignificant)")

p1 + p2/p3  

```

<!-- From Figure \ref{fig:gravitas-plot-8-id5}, it could further be observed that id5 has only one significant harmony $(hod, dow)$. Apparently, $(hod, wnd/wday)$ which is an important harmony for most households is not important for this one. -->

_Comparing households and validating patterns from linear display_ 


Figure \ref{fig:dotplot-8} helps to compare between households through the heatmap (b) and validate the results of the heatmap through linear display in (a) . The top panel contains the raw data for a month (Sep-2019). The darker the color of a cell in the heatmap, the more significant a harmony pair it is. Also, the ones with red borders are significant at 95% significance. This plot suggests that there is no significant periodic patterns for id 5, even when the demographics of id 5 is very similar to id 6 and 7. Household id 6 and 7 differ in the sense that for id 6, difference in patterns only during weekday/weekends (two members are in mixed profession), whereas for id 7, all or few other days of the week are also important (members are in academia and hence more flexible routines). The periodic pattern is very similar to id 8, which has a very different demographic property. Both from the linear representation and heatmap, it is clear that id 2 and 3 are similar, both in term of periodic pattern as well as demographics.

```{r gravitas-plot-8-id5, message = FALSE, warning=FALSE, fig.cap="something", eval = FALSE}

id5_tsibble <- elec %>%
  filter(id== 5)

p1 <- id5_tsibble %>% 
   prob_plot("hour_day",
             "day_week",
             response = "kwh",
             plot_type = "violin",
             symmetric = FALSE,
             quantile_prob = seq(0.1, 0.9, 0.01),
             fill = "blue") +
  ggtitle("a) hod vs dow (Rank 1)") + 
  scale_colour_brewer(name = "", palette = "Set2") +
  theme(legend.position = "none",
        strip.text = element_text(size = 7, margin = margin()), axis.text.x = element_text(angle = 60, hjust=1))

p2 <- id5_tsibble %>% 
   prob_plot("hour_day",
             "wknd_wday",
             response = "kwh",
             plot_type = "violin",
             symmetric = FALSE,
             quantile_prob = seq(0.1, 0.9, 0.01),
             fill = "blue") +
  ggtitle("a) hod vs wdwnd (insignificant)") + 
  scale_colour_brewer(name = "", palette = "Set2") +
  theme(legend.position = "none",
        strip.text = element_text(size = 7, margin = margin()), axis.text.x = element_text(angle = 60, hjust=1))


p1 + p2
```



```{r rank-household-mclapply-8}
elec <- read_rds(here("paper/data/elec_all-8.rds")) %>% 
  dplyr::filter(date(reading_datetime) >= ymd("20190701"), date(reading_datetime) < ymd("20191231"), meter_id==1) %>% 
  select(-meter_id) %>% 
  rename("id" = "household_id",
         "date_time" = "reading_datetime")



library(tictoc)
#tic()
elec_split = elec %>% group_split(id)

elec_select_harmony = parallel::mclapply(1:8, function(x){
  
  data_id <-  elec_split %>% magrittr::extract2(x) %>% 
    as_tsibble(index = date_time)
  
harmonies <- data_id %>%
  harmony(
    ugran = "month",
    filter_in = "wknd_wday",
    filter_out = c("hhour", "fortnight")
  )

 hakear::select_harmonies(data_id,
   harmony_tbl = harmonies,
   response = kwh,
   nperm = 200,
   nsamp = 200
 )

}, mc.cores = parallel::detectCores() - 1, mc.preschedule = FALSE, mc.set.seed = FALSE)
#toc()
```


```{r elec_select_harmony-8}
elec_harmony_all <- elec_select_harmony %>% 
  bind_rows(.id = "id") %>% 
  mutate(facet_variable = case_when(
    facet_variable == "hour_day" ~ "hod" ,
    facet_variable == "day_month" ~ "dom" ,
    facet_variable == "day_week" ~ "dow" ,
    facet_variable == "week_month" ~ "wom" ,
    facet_variable == "wknd_wday" ~ "wdwnd"
  )) %>% 
  mutate(x_variable = case_when(
    x_variable == "hour_day" ~ "hod" ,
    x_variable == "day_month" ~ "dom" ,
    x_variable == "day_week" ~ "dow" ,
    x_variable == "week_month" ~ "wom" ,
    x_variable == "wknd_wday" ~ "wdwnd"
  )) %>% 
  mutate(id = paste("id", id, sep = " ")) %>% 
  group_by(id) %>% 
  mutate(rank = row_number())
  
```


```{r dotplot-8, fig.cap = "Harmony pairs are shown for all household ids. The darker the colour, the higher the importance of the harmony. Also, the ones bordered red are selected with 90 percent significance level. Visualizing the pairs in this way helps us to see the important cyclic granularities along the x-axis and facet along with the information that which households should be analyzed together." }
elec_sig_split <- elec_harmony_all %>%
  mutate(select_split =  str_split(select_harmony, " ", simplify = TRUE)[,2]) %>% 
  mutate(significant = case_when(
    select_split == "***" ~ "highest",
    select_split == "**" ~ "high",
    select_split == "*" ~ "medium",
    select_split == "" ~ "low"
  )) %>% 
   mutate(rank = case_when(
    select_split == "***" ~ paste(rank, "***", sep = " "),
    select_split == "**" ~  paste(rank, "**", sep = " "),
    select_split == "*" ~  paste(rank, "*", sep = " "),
    select_split == "" ~  paste(rank, "", sep = " ")
  ))


elec_sig_split$significant <- 
  factor(elec_sig_split$significant, levels = c("highest", "high", "medium", "low"))

heatplot <- elec_sig_split %>% 
  mutate(significance_95 = if_else(significant %in% c("high", "highest"), "yes", "no")) %>% 
  ggplot() +
  geom_tile(aes(x = x_variable,
                 y = facet_variable, 
                 fill = wpd, color = significance_95), size = 1) + 
  #scale_fill_gradient(low = "white",high = "steelblue") +
  scale_fill_gradient2() +
  scale_colour_manual(values = c("white","red")) + 
  theme(legend.position = "bottom") +
  facet_wrap(~id, nrow = 1, strip.position = "bottom") +
  coord_fixed() + 
  guides(fill = guide_legend()) +
  theme_void() +
  theme_gray(base_size = 12, base_family = "Times") +
  theme(panel.grid = element_blank(),
        axis.text.x = element_text(angle = 60, hjust=1),legend.position = "bottom") + ggtitle("b")
  
elec <- read_rds(here("paper/data/elec_all-8.rds")) %>% 
  dplyr::filter(date(reading_datetime) >= ymd("20190701"), date(reading_datetime) < ymd("20191231"), meter_id==1) %>% 
  select(-meter_id) %>% 
  rename("id" = "household_id",
         "date_time" = "reading_datetime")

elec_zoom <-  elec %>%
  as_tibble() %>% 
  dplyr::filter(date(date_time) > as.Date("2019-09-01") & date(date_time) < (as.Date("2019-09-30"))) %>%
  ggplot(aes(x=date_time, y = kwh)) +
  #geom_point(size = 0.1, colour = "black", alpha = 0.3) +
  geom_line(size = 0.1, colour = "blue", alpha = 0.2) +
  facet_wrap(~id, 
             scales = "free_y",
             nrow = 1,
             strip.position =  "top") + xlab("") + 
  theme_void() + theme(strip.text.x = element_blank()) +
  ylab("energy demand in September (kwh)") + ggtitle("a")

elec_zoom/heatplot
  
#            
# dotplot <- elec_sig_split %>% 
#   ggplot() +
#   geom_point(aes(x = x_variable,
#                  y = facet_variable, 
#                  size = wpd, 
#                  fill = significant),
#              shape=21,
#              alpha= 0.8) +
#   scale_size_continuous() +
#   theme(legend.position = "bottom") +
#   scale_fill_viridis(discrete = TRUE, direction = 1) +
#   facet_wrap(~id, nrow = 2) +
#   guides(fill = guide_legend(), 
#          size = guide_legend()) +
#   theme_gray(base_size = 12, base_family = "Times") +
#    theme(legend.position = "bottom")
```


```{r tab-rank-8}
elec_rank <- elec_sig_split %>% 
  select(-c(6,7, 9, 10)) %>% 
  pivot_wider(
              names_from = id,
                values_from = rank) %>% 
  rename("facet variable" = "facet_variable",
         "x variable" = "x_variable",
         "facet levels" = "x_levels",
         "x levels" = "x_levels")

elec_rank %>% kable(format = "markdown", caption = "Ranking of harmonies for the eight households with significance levels.")
```


# Discussion

Exploratory data analysis involve many iterations of finding and summarizing patterns. With temporal data available at more and more finer scales, exploring periodicity has become overwhelming with so many possible granularities to explore. A common solution would be to zoom into “interesting” segments, but there is no way to know the “interesting” segments a priori. This work refines the search of periodic patterns by identifying those for which the differences between the displayed distributions is greatest, and rating them in order of importance for exploration.

A future direction of work could be to look at more individuals/subjects and group them according to similar periodic behavior. Behaviors across different harmonies would be varying for subjects and it would be hard to track the behavior when the number of individuals rise. One way to find groups would be to actually locate clusters who have similar periodic behavior.

<!-- patterns are interesting and others are not. The subjects for which behaviors across a group of cyclic granularities are similar, could be grouped together as their periodic behavior is similar.  to group subjects  -->

# Bibliography

```{r write-bib, eval = FALSE}
write_bib(c("rmarkdown", "bookdown", "rticles", "knitr", "ggplot2", "dplyr", "tsibble", "gravitas", "ggpubr", "kableExtra", "magrittr", "readr", "lubridate", "renv", "archive", "hakear", "patchwork", "ggpubr"), file = "rpkgs.bib")
```

