---
title: Median Maximum Pairwise Distance
authors:
- name: Sayani Gupta
  affiliation: Department of Econometrics and Business Statistics, Monash University
  email: Sayani.Gupta@monash.edu

bibliography: bibliography.bib
output:
  bookdown::pdf_book:
    #base_format: rticles::asa_article
    fig_height: 5
    fig_width: 8
    fig_caption: yes
    dev: "pdf"
---

```{r initial, echo = FALSE, cache = FALSE, include = FALSE}
options("knitr.graphics.auto_pdf" = TRUE)
library(knitr)
library(tidyverse)
library(lubridate)
library(lvplot)
library(ggridges)
library(viridis)
library(tsibble)
library(gravitas)
library(ggpubr)
library(readr)

opts_chunk$set(
  echo = FALSE, warning = FALSE, message = FALSE, comment = "#>",
  fig.path = "figure/", fig.align = "center", fig.show = "hold",
  cache = TRUE, cache.path = "cache/",
  out.width = ifelse(is_html_output(), "100%", "\\textwidth")
)
knitr::opts_knit$set(root.dir = here::here())
read_chunk('main.R')
read_chunk("null_distribution.R")
read_chunk("mean_null.R")
```

```{r external, include = FALSE}

```

```{r load}

```


```{r load_fun_1var}

```


```{r load_fun_2var}

```

```{r rank_harmony}

```

```{r global_threshold}

```

```{r}
set.seed(321)
```


# Introduction


Take the example of the calendar display of electricity smart meter data used in @wang2020calendar for four households in Melbourne, Australia. The authors show how hour-of-the-day interact with weekday and weekends and then move on to use calendar display to display the daily schedules. The calendar display has several components in it which helps us to look at energy consumption across hour-of-the-day, day-of-the-week, week-of-the-month and month-of-the-year all at once.
Some interaction of these cyclic granularities[@our-paper] could also be interpreted from this display. However, there are two things that might be of concern. Due to the combination of linear and cyclic representation of time, it is difficult to comprehend the interaction of periodicities at once from this display. For example, this display might not be the best to understand how hour-of-the-day or month-of-year varies across week-of-the-month. Further, it is not clear what all interactions of cyclic granularities should be read from this display as there could be many combinations that one can look at. Moreover, calendar effects are not restricted to conventional day-of-week or month-of-year deconstructions (@our-paper) and could include other cyclic granularities like hour-of-week or day-of-fortnight which could potentially become useful depending on context. Lastly, there might be certain interactions which are interesting and others which are not. It could be potentially useful to know which all cyclic granularities could be useful.

<!-- Take an example of a data set which are observed at fine temporal scales, like that of NYC bike usage available at https://www.citibikenyc.com/system-data. We use the `nyc_bikes` data set from the R package `tsibbledata` which takes a sample of 10 bikes for the year 2018. The `start_time` and the `stop_time` are recorded to a fineness of seconds. We can look at pair of cyclic granularities (hour_day, wknd_wday) or (week_month, day_week) to see how these periodicities interact. But there could be other pairs that are important too. How to understand which pairs are sufficient to explore given the data set without losing much information about the data. -->

<!-- When we need to understand the interplay of different periodicities in a high frequency temporal datasets, we have many choices to consider. In [@wang2019tsibble] and [@wang2020calendar], periodicities are explored across hour of the day and day of the week or months. But calendar effects are not restricted to conventional day-of-week or month-of-year deconstructions. -->

The paper (gravitas) describes how we can start with all possible combinations of cyclic time granularities and narrow down our search to harmonies and remove clashes without losing any perspectives about the data. Even after excluding clashes, the list of harmonies left could be large and overwhelming for human consumption. Hence, there is a need to rank the harmonies basis how well they capture the variation in the measured variable and additionally reduce the number of harmonies for further exploration/visualization. Assuming a numeric response variable, our graphics are displays of distributions compared across combinations of categorical variables, one placed at x-axis and the other on the facet. Displays that capture more variation within different categories in the same group would be important to bring out different patterns of the data. Here, we have two main objectives:

- To choose harmonies for which distributions of categories are significantly different 
- To rank the selected harmonies from highest to lowest variation in the distribution of their categories. The idea here is to rate a harmony pair higher if this variation between different levels of the x-axis variable is higher on an average across all levels of facet variables.

To be able to dice time in all possible ways in order to have multiple perspectives about peridicities in the data, there are too many things that we should look at ideally. Exploratory analysis is detective work and it is difficult to traverse through all possible ways without any systematic approach. It is immensely useful to be able to make transition from all possible ways to only ways that could potentially be important given a situation. 

Talk about Scagnostics: Tukey

# Computing distance measure

## Distance between distributions

<!-- One of the most important class of divergence is the f-divergence and includes measures like Kullback-Leibler divergence, Hellinger distance etc. The continuous version of f -divergence is given by -->
<!-- $$D_f(P||Q) := \int q(x)f(\frac{p(x)}{q(x)})$$, where -->
<!-- $f : [0,\infty) \rightarrow R \cup \{\infty\}$ is a continuous convex function, and $f(1) = 0$.  -->

  
The most common divergence measure between distributions is the Kullback-Leibler (KL) divergence introduced by Solomon Kullback and Richard Leibler in 1951. The KL divergence, denoted $D(p(x), q(x))$ is a non-symmetric measure of the difference between two probability distributions $p(x)$ and $q(x)$ and is interpreted as the amount of information lost when $q(x)$ is used to approximate $p(x)$. Although the KL divergence measures the “distance” between two distributions, it is not a distance measure since it is not symmetric. The Jensen-Shannon divergence based on the Kullback-Leibler divergence is symmetric and it always has a finite value. The square root of the Jensen-Shannon divergence is a metric, often referred to as Jensen-Shannon distance. Other common measures of distance are Hellinger distance, total variation distance and Fisher information metric. 

In the context of this paper, the pairwise distances between the distributions of the measured variable are computed through Jensen-Shannon distance which is based on Kullback-Leibler divergence and is defined by,

$$JSD(P||Q) = \frac{1}{2}D(P||M) + \frac{1}{2}D(Q||M)$$
where $M = \frac{P+Q}{2}$ and 
$D(P||Q) := \int^\infty_{-\infty} p(x)f(\frac{p(x)}{q(x)})$ is the KL divergence between distributions $p(x)$ and $q(x)$. Probability distributions are estimated through quantiles instead of kernel density so that there is minimal dependency on selecting kernel or bandwidth.

<!-- The Jensen-Shanon distance between two probability distribution $p_1$ and $p_2$ is given by $$d = [D(p_1, r) + D(p_2, r)]/2 \quad where \quad r = (p_1 + p_2)/2$$ where, -->
<!-- $$D(p_1,p_2) = \int^{\infty}_{-\infty}p_1(x)log\frac{p_1(x)}{p_2(x)}\,dx$$ is the Kullback-Leibler divergence between $p_1$ and $p_2$.   -->

<!-- We call this measure of variation as  Median Maximum Pairwise Distances (MMPD). -->

## Normalize distances

<!-- The harmony pairs could be arranged from highest to lowest average maximum pairwise distances across different levels of the harmonies. -->


Maximum pairwise distances are not robust to the number of levels and is higher for harmonies with higher levels. Thus these maximum pairwise distances need to be normalized for different harmonies in a way that eliminates the effect of different levels. The Fisher–Tippett–Gnedenko theorem in the field of Extreme Value Theory states that the maximum of a sample of iid random variables after proper re-normalization can converge in distribution to only one of Weibull, Gumbel or Freschet distribution, independent of the underlying data or process.

More formally, $d_{1},d_{2}\ldots ,d_{n}$ be a sequence of independent and identically-distributed pairwise distances and $M_{n}=\max\{d_{1},\ldots ,d_{n}\}$. Then Fisher–Tippett–Gnedenko theorem [@De_Haan2007-yx] suggests that if a sequence of pairs of real numbers $(a_{n}, b_{n})$ exists such that each $a_{n}>0$ and $\lim _{{m\to \infty }}P\left({\frac  {M_{n}-b_{n}}{a_{n}}}\leq x\right)=F(x)$, where $F$ is a non-degenerate distribution function, then the limit distribution $F$ belongs to either the Gumbel, Fréchet or Weibull family. The normalizing constants $(a_{n}, b_{n})$ vary depending on the underlying distribution of the pairwise distances. Hence to normalize appropriately, it is 
important to assume a distribution of these distances. 

### why normalize

Suppose that $X_1$, $X_2$ , ... , $X_n$ are i.i.d. random variables with expected values $E(X_i) = \mu < \infty$ and variance $Var(X_i) = \sigma^2 < \infty$. Let 
$Y = max(X_1, X_2, \dots, X_n)$.

Let $F_X(x)$ be the common distribution of the variables $X_i$ and let $F_Y(y)$ be the corresponding distribution of $Y$. $F_Y(y)$ could be obtained from $F_X(x)$ simply by using:
$F_Y(y) = P[(X_1 \leq y)\cap(X_2 \leq y) \cap...\cap (X_n \leq y)] = {F_X(y)}^n$. For large $n$, the distribution of $Y$ approaches a standard shape, which does not depend on $F_X$. But what about the case when $n$ is not large enough? The distribution of maximum in that case will indeed depend on $n$ and the underlying distribution of $X$. If $F_X(x)$ is the CDF of $X$, then $F_Y(y) = {F_x(y)}^n$. Suppose $\Phi$ nd $\phi$ are the cdf and pdf of a standard normal distribution, then
$f_Y(y) = n{\Phi(y)}^{n-1}\phi(y)$, which depends on $n$. Hence, we are trying to normalise for $n$. Also, it depends on the underlying distribution of $X$, which we have assumed as normal in our case. As $n$ grows, we  can see the right tail growing, which implies that the probability that we will get a higher maximum is more. Now, for large $n$, we used EVT to normalise for $n$, that is, we brought them to the same scale without distorting the range of the distribution. But in our case, we will mostly have small $n$. It is important to ensure that they have the same mean and variation, for being able to compare the maximum value across $n$. We observe from the following graphs that our normalisation works after $n=6$, after which the difference in mean and standard deviation flattens out a lot.



```{r create_simdata}

```

```{r create_nlevels}

```


```{r norm_max_new}

```

## Mean and standard deviation of the distribution of maximum


```{r}
library(ggridges)
library(viridis)
max_all_data <- create_nlevels(nlevels = seq(2, 30, 2),
                           nsim = 500,
                           sim_dist = distributional::dist_normal(5, 10), create_fun = max)

g1 <- max_all_data %>% ggplot() + geom_density_ridges(aes(x=sim_data, y = as.factor(ind))) + ylab("n") + xlab("max")

median_all_data <- create_nlevels(nlevels = seq(2, 30, 2),
                           nsim = 500,
                           sim_dist = distributional::dist_normal(5, 10), create_fun = median)

g2 <- max_all_data %>% ggplot() + geom_boxplot(aes(x=sim_data, y = as.factor(ind)))  + ylab("n") + xlab("median")

ggpubr::ggarrange(g1, g2, ncol = 2)


```

## Distribution of distances

### Theoretical evidence

JS distances are distributed as chi-squared with $m$ df where we discretize the continuous distribution with $m$ discrete values. Taking sample percentiles to approximate the integral would mean taking $m = 99$.
With large $m$, chi-squared is asymptotically normal by the CLT. Thus, by CLT, ${\chi^2}_{m} \tilde{} N(m, 2m)$, which would depend on the number of discretization used to approximate the continuous distribution. Then $b_n = 1-1/n$ quantile of the normal distribution and $a_n = 1/[n*\phi(b_n)]$ where $\phi$ is the normal density function. $n$ is the number of pairwise comparisons being made.

### Empirical evidence

Distribution of JS distances is assumed to be normal but the mean and variance are estimated from the sample, rather than deducing it from the number of discretization used to approximate the continuous distribution. We look at different scenarios, where observations are collected from Normal, Exponential, Chi-squared and Gumbel distribution and found the distribution of JS distances are similar, irrespective of which distribution they are drawn from.



#### Initial distribution of observed variables shown in plot title

<!-- ```{r distv11} -->

<!-- ``` -->

<!-- ```{r distv12} -->

<!-- ``` -->


<!-- ```{r distv13} -->

<!-- ``` -->


<!-- ```{r distv14} -->

<!-- ``` -->


## Median Maximum Pairwise distance

### Definition

### Algorithm for computation for all harmony pairs

\noindent The algorithm employed for computing MMPD is summarized as follows:
<!-- Algorithm -->

<!-- 1. Suppose $C_i$ and $C_2$ are two cyclic granularities such that $C_i$ maps index set to a set $\{A_1, A_2, A_3, \dots, A_l$\}, and $C_2$ maps index set to a set $\{B_1, B_2, B_3, \dots, B_m$\} and $v$ is the measured variable. Hence for each combination ($A_k$, $B_l$), we have the time series variable $v_{ij} \subseteq v$, $\forall i = {1, 2, \dots, l}$ and $\forall j = {1, 2, \dots, m}$ -->

- **Input:** Data corresponding to all harmony pairs, i.e., data sets of the form $(C_i, C_j, v)$ 
$\forall i, j \in N_C$  
- **Output:** MMPD (Median Maximum Pairwise Distances) measuring the average variation across different levels of $C_i$ and $C_j$ $\forall i, j \in N_C$  

1. Fix harmony pair $(C_i, C_j)$.

2. Fix $k$. Then there are $L$ groups corresponding to level $A_k$ of $C_i$.

3. Compute  $m = \binom{L}{2}$ pairwise distances between distributions of $L$ unordered levels and $m = L-1$ pairwise distances for $L$ ordered categories.

4. Identify maximum within the $m$ computed distances.

5. Compute normalized maximum distance ($NM$) using appropriate norming constants.
<!--so that the distribution of the normalized maximum converges to a Gumbel distribution as $m\rightarrow\infty$.-->


6. Use Steps 1-5 to compute normalized maximum distance for $\forall k \in  \{1, 2, \ldots, K\}$.

7. Compute MMPD = median $(NM_1, NM_2, \dots, NM_K)$/log($K$).

8. Repeat Steps 1 to 7 for all harmony pairs.

### Bounds

This is not correct becuase MMPD should be median of standardized Gumbel distribution. So no bound?

By @Lin1991-fj,
$$ 0 \leq JSD(P||Q) \leq ln(2)$$.

Thus,  $$ 0 \leq MMPD \leq \frac{ln(2)}{ln(k)}$$.
Now, by asumption $k \geq 2$ and hence,
<!-- \begin{equation} -->
<!-- For k = 2, $\frac{ln(2)}{ln(k)} = 1$  -->
<!-- \end{equation} -->
<!-- For $k > 2$, $\frac{ln(2)}{ln(k)}  <1$. -->


\[\frac{ln(2)}{ln(k)} : \left\{
  \begin{array}{lr}
    1 & if \ k = 2\\
    <1 & if \ k \ge 2
  \end{array}
\right.
\]

Thus, $$ 0 \leq MMPD \leq \ 1$$

# The statistical test

## Definition

### Algorithm for computation for all harmony pairs

**Assumption:** random permutation without considering ordering 
(global)

1. Given the data; $\{v_t: t=0, 1, 2, \dots, T-1\}$, the MMPD is computed and is represented by $MMPD_{obs}$.

2. From the original sequence a random permutation is obtained: $\{v_t^*: t=0, 1, 2, \dots, T-1\}$.

3. MMPD is computed for all random permutation of the data and is represented by $MMPD_{sample}$.

4.  Steps (2) and (3) are repeated a large number
of times M (e.g. 1000).

5. For each permutation, one $MMPD_{sample}$ value is obtained.

6. $95^{th}$ percentile of this $MMPD_{sample}$ distribution is computed and stored in $MMPD_{threshold}$.

7. If  $MMPD_{obs}> MMPD_{threshold}$, harmony pairs are accepted. Only one threshold for all harmony pairs.

Pros: Considering thresholds global for all harmony pairs would imply less computation time.

Cons: Only one threshold for all harmony pairs means we are assuming distribution of all harmonies pairs are similar, which might not be the case.But nevertheless, it is a good benchmark.


```{r, eval = FALSE, echo = FALSE}
smart_harmony <-read_rds("data/smart_harmony_nonst.rds")
smart_harmony
```



```{r smart_harmony,eval = FALSE, echo = FALSE}

sm <- smart_meter10 %>% dplyr::filter(customer_id %in% c("10017936"))

harmonies <- sm %>% 
  harmony(ugran = "month",
          filter_in = "wknd_wday",
          filter_out = c("hhour", "fortnight"))


harmony_tbl =  harmonies

smart_harmony <- sm %>% 
  rank_harmony(harmony_tbl = harmonies,
               response = "general_supply_kwh", 
               dist_ordered = TRUE)

smart_harmony %>% 
  mutate(MMPD = round(MMPD, 3), max_pd = round(max_pd, 3)) %>% 
  mutate(rankn = row_number()) %>%
  rename("rankun" = "r") %>% kable()
```

## Null distribution

### Normalised maximum distances follow standard Gumbel distribution

### Limiting distribution of median of normalised maximum distances is normal

Let a continuous population be given with cdf F(x) (cumulative distribution function) and median $\xi$ (assumed to exist uniquely). For a sample of size $2n + 1$, let $\tilde{x}$ denote the sample median. The distribution of $\tilde{x}$,under certain conditions, to be asymptotically normal with mean $\xi$ and variance $\sigma_n^2 = \frac{1}{4} [f(\xi)]^2(2n + 1)$, where $f(x) = F'(x)$ is the pdf (probability density function).


### Confidence interval of test statistic

## Simulation design 

Behavior of the statistic - control simulation

 - To check if different distributions impact
(simulate with different distributions but same for all levels)

 - To check if x-levels are normalised
(simulate with different distributions)
(simulate with different x-levels)


 - To check if facet-levels are normalised
(simulate with different distributions)
(simulate with different facet-levels for fixed x-levels)


## Size and power

To estimate the sampling distribution of the test statistic we need many samples generated under the null hypothesis. If the null hypothesis is true, changing the exposure would have no effect on the outcome. By randomly shuffling the exposures we can make up as many data sets as we like. If the null hypothesis is true the shuffled data sets should look like the real data, otherwise they should look different from the
real data. The ranking of the real test statistic among the shuffled test statistics gives a p-value.

<!-- Consider two cyclic granularities $A$ and $B$ with $2$ and $3$categories. Thus, the harmony table consisting of all possible harmony pairs (assuming all pairs are harmonies), would look like the following: -->

```{r harmony_min}
# harmonies <- tibble::tibble(facet_variable = c("A", "B", "A", "C", "B", "C"),
#                             x_variable  = c("B","A", "C", "A", "C", "B"),
#                             facet_levels = c(2, 3, 2, 4, 3, 4),
#                             x_levels = c(3, 2, 4, 2, 4, 3))
# 
# harmonies <- tibble::tibble(facet_variable = c("A", "B"),
#                             x_variable  = c("B","A"),
#                             facet_levels = c(2, 3),
#                             x_levels = c(3, 2))
#                             
# harmonies %>% knitr::kable()
```

<!-- The output table has the value of MMPD (normalized median maximum pairwise distances), gt_MMPD(global threshold of MMPD indicator). -->

### Size: Simulated same distribution for all combinations of categories for all harmony pairs.

Failure to reject the null hypothesis when there is in fact no significant effect.

<!-- # ```{r samenull_2by4, eval = FALSE} -->
<!-- #  -->
<!-- # ``` -->

```{r normalv21_power}

```


```{r normalv22_power}

```

### Power: Simulated same distribution for all combinations of categories for all harmony pairs.



```{r normalv23_power}

```


```{r normalv24_power}

```


*Conclusion*: The test rejects the null hypothesis if distributions are different.

### Scenario 2: Simulated different distributions for all combinations of categories for harmony pairs for few levels.



```{r diffnull_2by4, eval = FALSE}

```


*Conclusion*: The test select the harmony pair for which distribution of x-axis categories are significantly different


### Scenario 3: Simulated different distributions for all combinations of categories for all harmony pairs with many levels.


```{r diffnull_7by11, eval = FALSE}
```

*Conclusion*: The test indicates that both harmony pairs do not have significant variation.


### Scenario 4: Simulated different distributions for all combinations of categories for all harmony pairs with many levels - very different distribution across x-axis


```{r diffnull_7by11normal, eval = FALSE}
```

*Conclusion*: The test indicates that only the first harmony pair has significant variation.


### Scenario 5: Simulated different distributions for all combinations of categories for all harmony pairs with many levels - very different distribution across facets


```{r diffnull_7by11normal2, eval = FALSE}
```


<!-- *Conclusion*:  -->



<!-- ## Scenario 4: Cumulative 3 levels with 2, 7 and 11 and testing level and power -->

<!-- ```{r samenull_3levels} -->

<!-- ``` -->


<!-- *Conclusion*: With 3 levels the test incorrectly chooses 1 harmony pair with similar distribution. The harmony pair which is displayed. -->

<!-- <!-- ```{r diffnull_3levels} --> -->

<!-- <!-- ``` --> -->

<!-- *Conclusion*: The test with MMPD selects just one pair, as opposed to the test with maximum. This needs to be checked against what we expect from the test. The harmony pairs which are selected (either through MMPD or maximum) are displayed. -->


<!-- # ```{r} -->
<!-- # harmonies <- tibble::tibble(facet_variable = c("A", "B"),x_variable  = c("B","A"), facet_levels = c(2, 3),x_levels = c(3, 2)) -->
<!-- #  -->
<!-- # har1 <- harmonies[1,] -->
<!-- #  -->
<!-- # sim_dist1 = c(rep(distributional::dist_normal(mu = 10, sigma = 5),2),rep(distributional::dist_exponential(10),2), rep(distributional::dist_weibull(0.5, 2),2)) -->
<!-- #  -->
<!-- # data1 <- sim_distharmony1(har1, sim_dist = sim_dist1) -->
<!-- # data1 -->
<!-- #  -->
<!-- # data1 %>% unnest(sim_dist) %>% -->
<!-- #   ggplot(aes(x = Var2, y = sim_dist)) + -->
<!-- #   facet_wrap(~Var1) + geom_boxplot() + ggtitle("Same distribution 2 by 3") -->
<!-- #  -->
<!-- # response = "sim_dist" -->
<!-- #  -->
<!-- # MMPD_distribution <- data1 %>% -->
<!-- #   select(-dist) %>%  -->
<!-- #   unnest(sim_dist) %>%  -->
<!-- #   list() %>%  -->
<!-- #   global_threshold(harmony_tbl = har1, -->
<!-- #                    response = "sim_dist", -->
<!-- #                    dist_distribution = "normal", -->
<!-- #                    dist_ordered = TRUE, -->
<!-- #                    create_gran_data = FALSE, nsamp = 20) -->
<!-- #  -->
<!-- # # as_tibble(sample_MMPD) %>% mutate(id = row_number()) %>%  -->
<!-- # #   ggplot() + geom_histogram(aes(x = value)) -->
<!-- #  -->
<!-- # ``` -->
<!-- #  -->

# Application

# Summary and discussion